{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bc97c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88a99c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:\\\\Users\\\\syeon\\\\!soyeon\\\\archive\\\\train_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f455f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TYPE_A' 'TYPE_B' 'TYPE_C']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>week</th>\n",
       "      <th>center_id</th>\n",
       "      <th>city_code</th>\n",
       "      <th>region_code</th>\n",
       "      <th>op_area</th>\n",
       "      <th>meal_id</th>\n",
       "      <th>category</th>\n",
       "      <th>cuisine</th>\n",
       "      <th>checkout_price</th>\n",
       "      <th>base_price</th>\n",
       "      <th>emailer_for_promotion</th>\n",
       "      <th>homepage_featured</th>\n",
       "      <th>num_orders</th>\n",
       "      <th>center_type_TYPE_A</th>\n",
       "      <th>center_type_TYPE_B</th>\n",
       "      <th>center_type_TYPE_C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1379560</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>647</td>\n",
       "      <td>56</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1885</td>\n",
       "      <td>Beverages</td>\n",
       "      <td>Thai</td>\n",
       "      <td>136.83</td>\n",
       "      <td>152.29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>177</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1466964</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>647</td>\n",
       "      <td>56</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1993</td>\n",
       "      <td>Beverages</td>\n",
       "      <td>Thai</td>\n",
       "      <td>136.83</td>\n",
       "      <td>135.83</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>270</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1346989</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>647</td>\n",
       "      <td>56</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2539</td>\n",
       "      <td>Beverages</td>\n",
       "      <td>Thai</td>\n",
       "      <td>134.86</td>\n",
       "      <td>135.86</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>189</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1338232</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>647</td>\n",
       "      <td>56</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2139</td>\n",
       "      <td>Beverages</td>\n",
       "      <td>Indian</td>\n",
       "      <td>339.50</td>\n",
       "      <td>437.53</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>54</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1448490</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>647</td>\n",
       "      <td>56</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2631</td>\n",
       "      <td>Beverages</td>\n",
       "      <td>Indian</td>\n",
       "      <td>243.50</td>\n",
       "      <td>242.50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456543</th>\n",
       "      <td>1271326</td>\n",
       "      <td>145</td>\n",
       "      <td>61</td>\n",
       "      <td>473</td>\n",
       "      <td>77</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1543</td>\n",
       "      <td>Desert</td>\n",
       "      <td>Indian</td>\n",
       "      <td>484.09</td>\n",
       "      <td>484.09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>68</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456544</th>\n",
       "      <td>1062036</td>\n",
       "      <td>145</td>\n",
       "      <td>61</td>\n",
       "      <td>473</td>\n",
       "      <td>77</td>\n",
       "      <td>4.5</td>\n",
       "      <td>2304</td>\n",
       "      <td>Desert</td>\n",
       "      <td>Indian</td>\n",
       "      <td>482.09</td>\n",
       "      <td>482.09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456545</th>\n",
       "      <td>1110849</td>\n",
       "      <td>145</td>\n",
       "      <td>61</td>\n",
       "      <td>473</td>\n",
       "      <td>77</td>\n",
       "      <td>4.5</td>\n",
       "      <td>2664</td>\n",
       "      <td>Salad</td>\n",
       "      <td>Italian</td>\n",
       "      <td>237.68</td>\n",
       "      <td>321.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>501</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456546</th>\n",
       "      <td>1147725</td>\n",
       "      <td>145</td>\n",
       "      <td>61</td>\n",
       "      <td>473</td>\n",
       "      <td>77</td>\n",
       "      <td>4.5</td>\n",
       "      <td>2569</td>\n",
       "      <td>Salad</td>\n",
       "      <td>Italian</td>\n",
       "      <td>243.50</td>\n",
       "      <td>313.34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>729</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456547</th>\n",
       "      <td>1361984</td>\n",
       "      <td>145</td>\n",
       "      <td>61</td>\n",
       "      <td>473</td>\n",
       "      <td>77</td>\n",
       "      <td>4.5</td>\n",
       "      <td>2490</td>\n",
       "      <td>Salad</td>\n",
       "      <td>Italian</td>\n",
       "      <td>292.03</td>\n",
       "      <td>290.03</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>162</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>456548 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  week  center_id  city_code  region_code  op_area  meal_id  \\\n",
       "0       1379560     1         55        647           56      2.0     1885   \n",
       "1       1466964     1         55        647           56      2.0     1993   \n",
       "2       1346989     1         55        647           56      2.0     2539   \n",
       "3       1338232     1         55        647           56      2.0     2139   \n",
       "4       1448490     1         55        647           56      2.0     2631   \n",
       "...         ...   ...        ...        ...          ...      ...      ...   \n",
       "456543  1271326   145         61        473           77      4.5     1543   \n",
       "456544  1062036   145         61        473           77      4.5     2304   \n",
       "456545  1110849   145         61        473           77      4.5     2664   \n",
       "456546  1147725   145         61        473           77      4.5     2569   \n",
       "456547  1361984   145         61        473           77      4.5     2490   \n",
       "\n",
       "         category  cuisine  checkout_price  base_price  emailer_for_promotion  \\\n",
       "0       Beverages     Thai          136.83      152.29                      0   \n",
       "1       Beverages     Thai          136.83      135.83                      0   \n",
       "2       Beverages     Thai          134.86      135.86                      0   \n",
       "3       Beverages   Indian          339.50      437.53                      0   \n",
       "4       Beverages   Indian          243.50      242.50                      0   \n",
       "...           ...      ...             ...         ...                    ...   \n",
       "456543     Desert   Indian          484.09      484.09                      0   \n",
       "456544     Desert   Indian          482.09      482.09                      0   \n",
       "456545      Salad  Italian          237.68      321.07                      0   \n",
       "456546      Salad  Italian          243.50      313.34                      0   \n",
       "456547      Salad  Italian          292.03      290.03                      0   \n",
       "\n",
       "        homepage_featured  num_orders  center_type_TYPE_A  center_type_TYPE_B  \\\n",
       "0                       0         177                 0.0                 0.0   \n",
       "1                       0         270                 0.0                 0.0   \n",
       "2                       0         189                 0.0                 0.0   \n",
       "3                       0          54                 0.0                 0.0   \n",
       "4                       0          40                 0.0                 0.0   \n",
       "...                   ...         ...                 ...                 ...   \n",
       "456543                  0          68                 1.0                 0.0   \n",
       "456544                  0          42                 1.0                 0.0   \n",
       "456545                  0         501                 1.0                 0.0   \n",
       "456546                  0         729                 1.0                 0.0   \n",
       "456547                  0         162                 1.0                 0.0   \n",
       "\n",
       "        center_type_TYPE_C  \n",
       "0                      1.0  \n",
       "1                      1.0  \n",
       "2                      1.0  \n",
       "3                      1.0  \n",
       "4                      1.0  \n",
       "...                    ...  \n",
       "456543                 0.0  \n",
       "456544                 0.0  \n",
       "456545                 0.0  \n",
       "456546                 0.0  \n",
       "456547                 0.0  \n",
       "\n",
       "[456548 rows x 17 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "# fit_transform은 train에만 사용하고 test에는 학습된 인코더에 fit만 해야한다\n",
    "data_cat = ohe.fit_transform(df[['center_type']])\n",
    "data_cat\n",
    "\n",
    "#카테고리 확인\n",
    "print(ohe.categories_[0])\n",
    "\n",
    "#더미 변수 처럼 컬럼을 생성함 (카테고리명으로)\n",
    "#pd.DataFrame(data_cat, columns=['cat1_' + col for col in ohe.categories_[0]])\n",
    "\n",
    "#그리고나서 원본데이터에 부착함\n",
    "df_center = pd.concat([df.drop(columns=['center_type']),pd.DataFrame(data_cat, columns=['center_type_' + str(col) for col in ohe.categories_[0]])], axis=1)\n",
    "df_center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57bd267b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Beverages' 'Biryani' 'Desert' 'Extras' 'Fish' 'Other Snacks' 'Pasta'\n",
      " 'Pizza' 'Rice Bowl' 'Salad' 'Sandwich' 'Seafood' 'Soup' 'Starters']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>week</th>\n",
       "      <th>center_id</th>\n",
       "      <th>city_code</th>\n",
       "      <th>region_code</th>\n",
       "      <th>op_area</th>\n",
       "      <th>meal_id</th>\n",
       "      <th>cuisine</th>\n",
       "      <th>checkout_price</th>\n",
       "      <th>base_price</th>\n",
       "      <th>...</th>\n",
       "      <th>category_Fish</th>\n",
       "      <th>category_Other Snacks</th>\n",
       "      <th>category_Pasta</th>\n",
       "      <th>category_Pizza</th>\n",
       "      <th>category_Rice Bowl</th>\n",
       "      <th>category_Salad</th>\n",
       "      <th>category_Sandwich</th>\n",
       "      <th>category_Seafood</th>\n",
       "      <th>category_Soup</th>\n",
       "      <th>category_Starters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1379560</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>647</td>\n",
       "      <td>56</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1885</td>\n",
       "      <td>Thai</td>\n",
       "      <td>136.83</td>\n",
       "      <td>152.29</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1466964</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>647</td>\n",
       "      <td>56</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1993</td>\n",
       "      <td>Thai</td>\n",
       "      <td>136.83</td>\n",
       "      <td>135.83</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1346989</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>647</td>\n",
       "      <td>56</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2539</td>\n",
       "      <td>Thai</td>\n",
       "      <td>134.86</td>\n",
       "      <td>135.86</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1338232</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>647</td>\n",
       "      <td>56</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2139</td>\n",
       "      <td>Indian</td>\n",
       "      <td>339.50</td>\n",
       "      <td>437.53</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1448490</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>647</td>\n",
       "      <td>56</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2631</td>\n",
       "      <td>Indian</td>\n",
       "      <td>243.50</td>\n",
       "      <td>242.50</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456543</th>\n",
       "      <td>1271326</td>\n",
       "      <td>145</td>\n",
       "      <td>61</td>\n",
       "      <td>473</td>\n",
       "      <td>77</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1543</td>\n",
       "      <td>Indian</td>\n",
       "      <td>484.09</td>\n",
       "      <td>484.09</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456544</th>\n",
       "      <td>1062036</td>\n",
       "      <td>145</td>\n",
       "      <td>61</td>\n",
       "      <td>473</td>\n",
       "      <td>77</td>\n",
       "      <td>4.5</td>\n",
       "      <td>2304</td>\n",
       "      <td>Indian</td>\n",
       "      <td>482.09</td>\n",
       "      <td>482.09</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456545</th>\n",
       "      <td>1110849</td>\n",
       "      <td>145</td>\n",
       "      <td>61</td>\n",
       "      <td>473</td>\n",
       "      <td>77</td>\n",
       "      <td>4.5</td>\n",
       "      <td>2664</td>\n",
       "      <td>Italian</td>\n",
       "      <td>237.68</td>\n",
       "      <td>321.07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456546</th>\n",
       "      <td>1147725</td>\n",
       "      <td>145</td>\n",
       "      <td>61</td>\n",
       "      <td>473</td>\n",
       "      <td>77</td>\n",
       "      <td>4.5</td>\n",
       "      <td>2569</td>\n",
       "      <td>Italian</td>\n",
       "      <td>243.50</td>\n",
       "      <td>313.34</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456547</th>\n",
       "      <td>1361984</td>\n",
       "      <td>145</td>\n",
       "      <td>61</td>\n",
       "      <td>473</td>\n",
       "      <td>77</td>\n",
       "      <td>4.5</td>\n",
       "      <td>2490</td>\n",
       "      <td>Italian</td>\n",
       "      <td>292.03</td>\n",
       "      <td>290.03</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>456548 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  week  center_id  city_code  region_code  op_area  meal_id  \\\n",
       "0       1379560     1         55        647           56      2.0     1885   \n",
       "1       1466964     1         55        647           56      2.0     1993   \n",
       "2       1346989     1         55        647           56      2.0     2539   \n",
       "3       1338232     1         55        647           56      2.0     2139   \n",
       "4       1448490     1         55        647           56      2.0     2631   \n",
       "...         ...   ...        ...        ...          ...      ...      ...   \n",
       "456543  1271326   145         61        473           77      4.5     1543   \n",
       "456544  1062036   145         61        473           77      4.5     2304   \n",
       "456545  1110849   145         61        473           77      4.5     2664   \n",
       "456546  1147725   145         61        473           77      4.5     2569   \n",
       "456547  1361984   145         61        473           77      4.5     2490   \n",
       "\n",
       "        cuisine  checkout_price  base_price  ...  category_Fish  \\\n",
       "0          Thai          136.83      152.29  ...            0.0   \n",
       "1          Thai          136.83      135.83  ...            0.0   \n",
       "2          Thai          134.86      135.86  ...            0.0   \n",
       "3        Indian          339.50      437.53  ...            0.0   \n",
       "4        Indian          243.50      242.50  ...            0.0   \n",
       "...         ...             ...         ...  ...            ...   \n",
       "456543   Indian          484.09      484.09  ...            0.0   \n",
       "456544   Indian          482.09      482.09  ...            0.0   \n",
       "456545  Italian          237.68      321.07  ...            0.0   \n",
       "456546  Italian          243.50      313.34  ...            0.0   \n",
       "456547  Italian          292.03      290.03  ...            0.0   \n",
       "\n",
       "        category_Other Snacks  category_Pasta  category_Pizza  \\\n",
       "0                         0.0             0.0             0.0   \n",
       "1                         0.0             0.0             0.0   \n",
       "2                         0.0             0.0             0.0   \n",
       "3                         0.0             0.0             0.0   \n",
       "4                         0.0             0.0             0.0   \n",
       "...                       ...             ...             ...   \n",
       "456543                    0.0             0.0             0.0   \n",
       "456544                    0.0             0.0             0.0   \n",
       "456545                    0.0             0.0             0.0   \n",
       "456546                    0.0             0.0             0.0   \n",
       "456547                    0.0             0.0             0.0   \n",
       "\n",
       "        category_Rice Bowl  category_Salad  category_Sandwich  \\\n",
       "0                      0.0             0.0                0.0   \n",
       "1                      0.0             0.0                0.0   \n",
       "2                      0.0             0.0                0.0   \n",
       "3                      0.0             0.0                0.0   \n",
       "4                      0.0             0.0                0.0   \n",
       "...                    ...             ...                ...   \n",
       "456543                 0.0             0.0                0.0   \n",
       "456544                 0.0             0.0                0.0   \n",
       "456545                 0.0             1.0                0.0   \n",
       "456546                 0.0             1.0                0.0   \n",
       "456547                 0.0             1.0                0.0   \n",
       "\n",
       "        category_Seafood  category_Soup  category_Starters  \n",
       "0                    0.0            0.0                0.0  \n",
       "1                    0.0            0.0                0.0  \n",
       "2                    0.0            0.0                0.0  \n",
       "3                    0.0            0.0                0.0  \n",
       "4                    0.0            0.0                0.0  \n",
       "...                  ...            ...                ...  \n",
       "456543               0.0            0.0                0.0  \n",
       "456544               0.0            0.0                0.0  \n",
       "456545               0.0            0.0                0.0  \n",
       "456546               0.0            0.0                0.0  \n",
       "456547               0.0            0.0                0.0  \n",
       "\n",
       "[456548 rows x 30 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cat = ohe.fit_transform(df[['category']])\n",
    "data_cat\n",
    "\n",
    "#카테고리 확인\n",
    "print(ohe.categories_[0])\n",
    "\n",
    "#더미 변수 처럼 컬럼을 생성함 (카테고리명으로)\n",
    "#pd.DataFrame(data_cat, columns=['cat1_' + col for col in ohe.categories_[0]])\n",
    "\n",
    "#그리고나서 원본데이터에 부착함\n",
    "df_category = pd.concat([df_center.drop(columns=['category']),pd.DataFrame(data_cat, columns=['category_' + str(col) for col in ohe.categories_[0]])], axis=1)\n",
    "df_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "233a31a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Continental' 'Indian' 'Italian' 'Thai']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>week</th>\n",
       "      <th>center_id</th>\n",
       "      <th>city_code</th>\n",
       "      <th>region_code</th>\n",
       "      <th>op_area</th>\n",
       "      <th>meal_id</th>\n",
       "      <th>checkout_price</th>\n",
       "      <th>base_price</th>\n",
       "      <th>emailer_for_promotion</th>\n",
       "      <th>...</th>\n",
       "      <th>category_Rice Bowl</th>\n",
       "      <th>category_Salad</th>\n",
       "      <th>category_Sandwich</th>\n",
       "      <th>category_Seafood</th>\n",
       "      <th>category_Soup</th>\n",
       "      <th>category_Starters</th>\n",
       "      <th>cuisine_Continental</th>\n",
       "      <th>cuisine_Indian</th>\n",
       "      <th>cuisine_Italian</th>\n",
       "      <th>cuisine_Thai</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1379560</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>647</td>\n",
       "      <td>56</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1885</td>\n",
       "      <td>136.83</td>\n",
       "      <td>152.29</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1466964</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>647</td>\n",
       "      <td>56</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1993</td>\n",
       "      <td>136.83</td>\n",
       "      <td>135.83</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1346989</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>647</td>\n",
       "      <td>56</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2539</td>\n",
       "      <td>134.86</td>\n",
       "      <td>135.86</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1338232</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>647</td>\n",
       "      <td>56</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2139</td>\n",
       "      <td>339.50</td>\n",
       "      <td>437.53</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1448490</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>647</td>\n",
       "      <td>56</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2631</td>\n",
       "      <td>243.50</td>\n",
       "      <td>242.50</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456543</th>\n",
       "      <td>1271326</td>\n",
       "      <td>145</td>\n",
       "      <td>61</td>\n",
       "      <td>473</td>\n",
       "      <td>77</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1543</td>\n",
       "      <td>484.09</td>\n",
       "      <td>484.09</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456544</th>\n",
       "      <td>1062036</td>\n",
       "      <td>145</td>\n",
       "      <td>61</td>\n",
       "      <td>473</td>\n",
       "      <td>77</td>\n",
       "      <td>4.5</td>\n",
       "      <td>2304</td>\n",
       "      <td>482.09</td>\n",
       "      <td>482.09</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456545</th>\n",
       "      <td>1110849</td>\n",
       "      <td>145</td>\n",
       "      <td>61</td>\n",
       "      <td>473</td>\n",
       "      <td>77</td>\n",
       "      <td>4.5</td>\n",
       "      <td>2664</td>\n",
       "      <td>237.68</td>\n",
       "      <td>321.07</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456546</th>\n",
       "      <td>1147725</td>\n",
       "      <td>145</td>\n",
       "      <td>61</td>\n",
       "      <td>473</td>\n",
       "      <td>77</td>\n",
       "      <td>4.5</td>\n",
       "      <td>2569</td>\n",
       "      <td>243.50</td>\n",
       "      <td>313.34</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456547</th>\n",
       "      <td>1361984</td>\n",
       "      <td>145</td>\n",
       "      <td>61</td>\n",
       "      <td>473</td>\n",
       "      <td>77</td>\n",
       "      <td>4.5</td>\n",
       "      <td>2490</td>\n",
       "      <td>292.03</td>\n",
       "      <td>290.03</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>456548 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  week  center_id  city_code  region_code  op_area  meal_id  \\\n",
       "0       1379560     1         55        647           56      2.0     1885   \n",
       "1       1466964     1         55        647           56      2.0     1993   \n",
       "2       1346989     1         55        647           56      2.0     2539   \n",
       "3       1338232     1         55        647           56      2.0     2139   \n",
       "4       1448490     1         55        647           56      2.0     2631   \n",
       "...         ...   ...        ...        ...          ...      ...      ...   \n",
       "456543  1271326   145         61        473           77      4.5     1543   \n",
       "456544  1062036   145         61        473           77      4.5     2304   \n",
       "456545  1110849   145         61        473           77      4.5     2664   \n",
       "456546  1147725   145         61        473           77      4.5     2569   \n",
       "456547  1361984   145         61        473           77      4.5     2490   \n",
       "\n",
       "        checkout_price  base_price  emailer_for_promotion  ...  \\\n",
       "0               136.83      152.29                      0  ...   \n",
       "1               136.83      135.83                      0  ...   \n",
       "2               134.86      135.86                      0  ...   \n",
       "3               339.50      437.53                      0  ...   \n",
       "4               243.50      242.50                      0  ...   \n",
       "...                ...         ...                    ...  ...   \n",
       "456543          484.09      484.09                      0  ...   \n",
       "456544          482.09      482.09                      0  ...   \n",
       "456545          237.68      321.07                      0  ...   \n",
       "456546          243.50      313.34                      0  ...   \n",
       "456547          292.03      290.03                      0  ...   \n",
       "\n",
       "        category_Rice Bowl  category_Salad  category_Sandwich  \\\n",
       "0                      0.0             0.0                0.0   \n",
       "1                      0.0             0.0                0.0   \n",
       "2                      0.0             0.0                0.0   \n",
       "3                      0.0             0.0                0.0   \n",
       "4                      0.0             0.0                0.0   \n",
       "...                    ...             ...                ...   \n",
       "456543                 0.0             0.0                0.0   \n",
       "456544                 0.0             0.0                0.0   \n",
       "456545                 0.0             1.0                0.0   \n",
       "456546                 0.0             1.0                0.0   \n",
       "456547                 0.0             1.0                0.0   \n",
       "\n",
       "        category_Seafood  category_Soup  category_Starters  \\\n",
       "0                    0.0            0.0                0.0   \n",
       "1                    0.0            0.0                0.0   \n",
       "2                    0.0            0.0                0.0   \n",
       "3                    0.0            0.0                0.0   \n",
       "4                    0.0            0.0                0.0   \n",
       "...                  ...            ...                ...   \n",
       "456543               0.0            0.0                0.0   \n",
       "456544               0.0            0.0                0.0   \n",
       "456545               0.0            0.0                0.0   \n",
       "456546               0.0            0.0                0.0   \n",
       "456547               0.0            0.0                0.0   \n",
       "\n",
       "        cuisine_Continental  cuisine_Indian  cuisine_Italian  cuisine_Thai  \n",
       "0                       0.0             0.0              0.0           1.0  \n",
       "1                       0.0             0.0              0.0           1.0  \n",
       "2                       0.0             0.0              0.0           1.0  \n",
       "3                       0.0             1.0              0.0           0.0  \n",
       "4                       0.0             1.0              0.0           0.0  \n",
       "...                     ...             ...              ...           ...  \n",
       "456543                  0.0             1.0              0.0           0.0  \n",
       "456544                  0.0             1.0              0.0           0.0  \n",
       "456545                  0.0             0.0              1.0           0.0  \n",
       "456546                  0.0             0.0              1.0           0.0  \n",
       "456547                  0.0             0.0              1.0           0.0  \n",
       "\n",
       "[456548 rows x 33 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cat = ohe.fit_transform(df[['cuisine']])\n",
    "data_cat\n",
    "\n",
    "#카테고리 확인\n",
    "print(ohe.categories_[0])\n",
    "\n",
    "#더미 변수 처럼 컬럼을 생성함 (카테고리명으로)\n",
    "#pd.DataFrame(data_cat, columns=['cat1_' + col for col in ohe.categories_[0]])\n",
    "\n",
    "#그리고나서 원본데이터에 부착함\n",
    "train_df = pd.concat([df_category.drop(columns=['cuisine']),pd.DataFrame(data_cat, columns=['cuisine_' + str(col) for col in ohe.categories_[0]])], axis=1)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d6465d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "388a5456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features와 target 분리\n",
    "X = train_df.drop('num_orders', axis=1)\n",
    "y = train_df['num_orders']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47e53549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, test를 8:2로 나누기\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8dde80d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MinMaxScaler: \n",
      "                   0              1              2              3   \\\n",
      "count  365238.000000  365238.000000  365238.000000  365238.000000   \n",
      "mean        0.500463       0.512199       0.409727       0.566405   \n",
      "std         0.288605       0.288422       0.261217       0.257554   \n",
      "min         0.000000       0.000000       0.000000       0.000000   \n",
      "25%         0.250648       0.263889       0.187500       0.377432   \n",
      "50%         0.500666       0.520833       0.375000       0.544747   \n",
      "75%         0.750459       0.763889       0.568182       0.758755   \n",
      "max         1.000000       1.000000       1.000000       1.000000   \n",
      "\n",
      "                  4              5              6              7   \\\n",
      "count  365238.000000  365238.000000  365238.000000  365238.000000   \n",
      "mean        0.480382       0.521916       0.508245       0.381322   \n",
      "std         0.251985       0.179139       0.289063       0.177079   \n",
      "min         0.000000       0.000000       0.000000       0.000000   \n",
      "25%         0.157143       0.442623       0.261880       0.261763   \n",
      "50%         0.471429       0.508197       0.491552       0.339326   \n",
      "75%         0.771429       0.590164       0.779831       0.512290   \n",
      "max         1.000000       1.000000       1.000000       1.000000   \n",
      "\n",
      "                  8              9   ...             22             23  \\\n",
      "count  365238.000000  365238.000000  ...  365238.000000  365238.000000   \n",
      "mean        0.368321       0.081136  ...       0.073169       0.062756   \n",
      "std         0.198021       0.273044  ...       0.260414       0.242525   \n",
      "min         0.000000       0.000000  ...       0.000000       0.000000   \n",
      "25%         0.232020       0.000000  ...       0.000000       0.000000   \n",
      "50%         0.314556       0.000000  ...       0.000000       0.000000   \n",
      "75%         0.497534       0.000000  ...       0.000000       0.000000   \n",
      "max         1.000000       1.000000  ...       1.000000       1.000000   \n",
      "\n",
      "                  24             25             26             27  \\\n",
      "count  365238.000000  365238.000000  365238.000000  365238.000000   \n",
      "mean        0.073024       0.058723       0.027886       0.065741   \n",
      "std         0.260176       0.235107       0.164646       0.247829   \n",
      "min         0.000000       0.000000       0.000000       0.000000   \n",
      "25%         0.000000       0.000000       0.000000       0.000000   \n",
      "50%         0.000000       0.000000       0.000000       0.000000   \n",
      "75%         0.000000       0.000000       0.000000       0.000000   \n",
      "max         1.000000       1.000000       1.000000       1.000000   \n",
      "\n",
      "                  28             29             30             31  \n",
      "count  365238.000000  365238.000000  365238.000000  365238.000000  \n",
      "mean        0.224722       0.246401       0.269413       0.259464  \n",
      "std         0.417400       0.430915       0.443656       0.438341  \n",
      "min         0.000000       0.000000       0.000000       0.000000  \n",
      "25%         0.000000       0.000000       0.000000       0.000000  \n",
      "50%         0.000000       0.000000       0.000000       0.000000  \n",
      "75%         0.000000       0.000000       1.000000       1.000000  \n",
      "max         1.000000       1.000000       1.000000       1.000000  \n",
      "\n",
      "[8 rows x 32 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train_x)\n",
    "X_train_transformed = pd.DataFrame(scaler.transform(train_x))\n",
    "print(\"\\nMinMaxScaler: \\n\", X_train_transformed.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8b39baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5개를 임포트하세요 (Dense, tf, seauential, Modelcheckpoint)\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c813c869",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(365238, 32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x의 입력 갯수 확인\n",
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c26f9ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델 정의 및 입력 (4개의 입력 받아, n개의 퍼셉트론 구성된 레이어 제작)\n",
    "model_dl = tf.keras.Sequential()\n",
    "model_dl.add(tf.keras.layers.Dense(128, activation = 'relu', input_shape = [32]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dcee7b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#덴스레이어 쌓기 (relu 또는 swish)\n",
    "model_dl.add(tf.keras.layers.Dense(64, activation = 'relu'))\n",
    "model_dl.add(tf.keras.layers.Dense(32, activation = 'relu'))\n",
    "model_dl.add(tf.keras.layers.Dense(16, activation = 'relu'))\n",
    "model_dl.add(tf.keras.layers.Dense(8, activation = 'relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8899925e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#마지막 출력 레이어 확인 (리그레션이라 없음)\n",
    "model_dl.add(tf.keras.layers.Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8646178b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 컴파일 지정하기\n",
    "optimizer = tf.keras.optimizers.RMSprop()\n",
    "model_dl.compile(optimizer,\n",
    "                 loss = 'mse',\n",
    "                 metrics = ['mse', 'mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "72a0d31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_17 (Dense)            (None, 128)               4224      \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 16)                528       \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15,233\n",
      "Trainable params: 15,233\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_dl.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e22aab32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11414/11414 [==============================] - 35s 3ms/step - loss: 578614.5000 - mse: 578614.5000 - mae: 269.2086 - val_loss: 159694.1562 - val_mse: 159694.1562 - val_mae: 208.8213\n",
      "Epoch 2/100\n",
      "11414/11414 [==============================] - 32s 3ms/step - loss: 157168.4062 - mse: 157168.4062 - mae: 223.7451 - val_loss: 156318.0469 - val_mse: 156318.0469 - val_mae: 226.5959\n",
      "Epoch 3/100\n",
      "11414/11414 [==============================] - 33s 3ms/step - loss: 156872.9219 - mse: 156872.9219 - mae: 226.6659 - val_loss: 156321.3594 - val_mse: 156321.3594 - val_mae: 225.9250\n",
      "Epoch 4/100\n",
      "11414/11414 [==============================] - 36s 3ms/step - loss: 156871.6094 - mse: 156871.6094 - mae: 226.6977 - val_loss: 156325.3438 - val_mse: 156325.3438 - val_mae: 225.6010\n",
      "Epoch 5/100\n",
      "11414/11414 [==============================] - 33s 3ms/step - loss: 156870.7031 - mse: 156870.7031 - mae: 226.7529 - val_loss: 156320.0156 - val_mse: 156320.0156 - val_mae: 226.0622\n",
      "Epoch 6/100\n",
      "11414/11414 [==============================] - 33s 3ms/step - loss: 156868.7031 - mse: 156868.7031 - mae: 226.8051 - val_loss: 156326.6406 - val_mse: 156326.6406 - val_mae: 225.5171\n",
      "Epoch 7/100\n",
      "11414/11414 [==============================] - 31s 3ms/step - loss: 156873.6719 - mse: 156873.6719 - mae: 226.5833 - val_loss: 156318.0625 - val_mse: 156318.0625 - val_mae: 226.5111\n",
      "Epoch 8/100\n",
      "11414/11414 [==============================] - 30s 3ms/step - loss: 156871.7344 - mse: 156871.7344 - mae: 226.6821 - val_loss: 156321.2344 - val_mse: 156321.2344 - val_mae: 225.9555\n",
      "Epoch 9/100\n",
      "11414/11414 [==============================] - 30s 3ms/step - loss: 156872.7500 - mse: 156872.7500 - mae: 226.5920 - val_loss: 156319.9219 - val_mse: 156319.9219 - val_mae: 226.1180\n",
      "Epoch 10/100\n",
      "11414/11414 [==============================] - 30s 3ms/step - loss: 156872.9688 - mse: 156872.9688 - mae: 226.6451 - val_loss: 156326.8906 - val_mse: 156326.8906 - val_mae: 225.4949\n",
      "Epoch 11/100\n",
      "11414/11414 [==============================] - 30s 3ms/step - loss: 156874.2188 - mse: 156874.2188 - mae: 226.5441 - val_loss: 156317.9844 - val_mse: 156317.9844 - val_mae: 226.6615\n",
      "Epoch 12/100\n",
      "11414/11414 [==============================] - 30s 3ms/step - loss: 156870.4688 - mse: 156870.4688 - mae: 226.8099 - val_loss: 156325.2812 - val_mse: 156325.2812 - val_mae: 225.6052\n",
      "Epoch 13/100\n",
      "11414/11414 [==============================] - 30s 3ms/step - loss: 156874.8438 - mse: 156874.8438 - mae: 226.5565 - val_loss: 156318.6406 - val_mse: 156318.6406 - val_mae: 226.3345\n",
      "Epoch 14/100\n",
      "11414/11414 [==============================] - 30s 3ms/step - loss: 156872.2500 - mse: 156872.2500 - mae: 226.6691 - val_loss: 156322.0156 - val_mse: 156322.0156 - val_mae: 225.8479\n",
      "Epoch 15/100\n",
      "11414/11414 [==============================] - 30s 3ms/step - loss: 156875.1406 - mse: 156875.1406 - mae: 226.4907 - val_loss: 156318.9844 - val_mse: 156318.9844 - val_mae: 226.2854\n",
      "Epoch 16/100\n",
      "11414/11414 [==============================] - 30s 3ms/step - loss: 156873.1250 - mse: 156873.1250 - mae: 226.6477 - val_loss: 156319.7031 - val_mse: 156319.7031 - val_mae: 226.1280\n",
      "Epoch 17/100\n",
      "11414/11414 [==============================] - 33s 3ms/step - loss: 156872.4062 - mse: 156872.4062 - mae: 226.6721 - val_loss: 156319.0312 - val_mse: 156319.0312 - val_mae: 226.2604\n",
      "Epoch 18/100\n",
      "11414/11414 [==============================] - 31s 3ms/step - loss: 156873.4062 - mse: 156873.4062 - mae: 226.5789 - val_loss: 156317.8906 - val_mse: 156317.8906 - val_mae: 226.6747\n",
      "Epoch 19/100\n",
      "11414/11414 [==============================] - 30s 3ms/step - loss: 156871.6250 - mse: 156871.6250 - mae: 226.7394 - val_loss: 156320.9688 - val_mse: 156320.9688 - val_mae: 225.9741\n",
      "Epoch 20/100\n",
      "11414/11414 [==============================] - 31s 3ms/step - loss: 156873.0781 - mse: 156873.0781 - mae: 226.6501 - val_loss: 156319.4375 - val_mse: 156319.4375 - val_mae: 226.1661\n",
      "Epoch 21/100\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 156871.7344 - mse: 156871.7344 - mae: 226.6515 - val_loss: 156320.0156 - val_mse: 156320.0156 - val_mae: 226.0599\n",
      "Epoch 22/100\n",
      "11414/11414 [==============================] - 30s 3ms/step - loss: 156873.4375 - mse: 156873.4375 - mae: 226.6262 - val_loss: 156319.8906 - val_mse: 156319.8906 - val_mae: 226.1002\n",
      "Epoch 23/100\n",
      "11414/11414 [==============================] - 30s 3ms/step - loss: 156870.7969 - mse: 156870.7969 - mae: 226.6227 - val_loss: 156319.0156 - val_mse: 156319.0156 - val_mae: 226.2832\n",
      "Epoch 24/100\n",
      "11414/11414 [==============================] - 30s 3ms/step - loss: 156872.4688 - mse: 156872.4688 - mae: 226.6313 - val_loss: 156319.7500 - val_mse: 156319.7500 - val_mae: 226.1391\n",
      "Epoch 25/100\n",
      "11414/11414 [==============================] - 30s 3ms/step - loss: 156872.5938 - mse: 156872.5938 - mae: 226.6772 - val_loss: 156319.3281 - val_mse: 156319.3281 - val_mae: 226.1711\n",
      "Epoch 26/100\n",
      "11414/11414 [==============================] - 30s 3ms/step - loss: 156869.2188 - mse: 156869.2188 - mae: 226.7897 - val_loss: 156325.8281 - val_mse: 156325.8281 - val_mae: 225.5683\n",
      "Epoch 27/100\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 156874.1875 - mse: 156874.1875 - mae: 226.5386 - val_loss: 156319.0156 - val_mse: 156319.0156 - val_mae: 226.2121\n",
      "Epoch 28/100\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 156873.7969 - mse: 156873.7969 - mae: 226.5876 - val_loss: 156318.3438 - val_mse: 156318.3438 - val_mae: 226.3676\n",
      "Epoch 29/100\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 156870.7969 - mse: 156870.7969 - mae: 226.7311 - val_loss: 156321.4219 - val_mse: 156321.4219 - val_mae: 225.9288\n",
      "Epoch 30/100\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 156874.0781 - mse: 156874.0781 - mae: 226.5613 - val_loss: 156319.6875 - val_mse: 156319.6875 - val_mae: 226.1306\n",
      "Epoch 31/100\n",
      "11414/11414 [==============================] - 30s 3ms/step - loss: 156873.7188 - mse: 156873.7188 - mae: 226.5379 - val_loss: 156319.0312 - val_mse: 156319.0312 - val_mae: 226.2656\n",
      "Epoch 32/100\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 156871.7188 - mse: 156871.7188 - mae: 226.7108 - val_loss: 156320.0312 - val_mse: 156320.0312 - val_mae: 226.0828\n",
      "Epoch 33/100\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 156874.1250 - mse: 156874.1250 - mae: 226.5199 - val_loss: 156318.3750 - val_mse: 156318.3750 - val_mae: 226.4312\n",
      "Epoch 34/100\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 156870.2656 - mse: 156870.2656 - mae: 226.7670 - val_loss: 156319.9375 - val_mse: 156319.9375 - val_mae: 226.1093\n",
      "Epoch 35/100\n",
      "11414/11414 [==============================] - 30s 3ms/step - loss: 156871.2344 - mse: 156871.2344 - mae: 226.7512 - val_loss: 156323.3125 - val_mse: 156323.3125 - val_mae: 225.7406\n",
      "Epoch 36/100\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 156871.4688 - mse: 156871.4688 - mae: 226.6215 - val_loss: 156318.2188 - val_mse: 156318.2188 - val_mae: 226.7667\n",
      "Epoch 37/100\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 156871.1094 - mse: 156871.1094 - mae: 226.7462 - val_loss: 156320.1406 - val_mse: 156320.1406 - val_mae: 226.1104\n",
      "Epoch 38/100\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 156870.5469 - mse: 156870.5469 - mae: 226.7453 - val_loss: 156324.9844 - val_mse: 156324.9844 - val_mae: 225.6214\n",
      "Epoch 39/100\n",
      "11414/11414 [==============================] - 30s 3ms/step - loss: 156874.3125 - mse: 156874.3125 - mae: 226.5401 - val_loss: 156318.3438 - val_mse: 156318.3438 - val_mae: 226.3969\n",
      "Epoch 40/100\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 156873.5000 - mse: 156873.5000 - mae: 226.5833 - val_loss: 156318.0156 - val_mse: 156318.0156 - val_mae: 226.5046\n",
      "Epoch 41/100\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 156870.3594 - mse: 156870.3594 - mae: 226.6988 - val_loss: 156320.9688 - val_mse: 156320.9688 - val_mae: 225.9653\n",
      "Epoch 42/100\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 156871.6094 - mse: 156871.6094 - mae: 226.6651 - val_loss: 156318.2969 - val_mse: 156318.2969 - val_mae: 226.4058\n",
      "Epoch 43/100\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 156870.6875 - mse: 156870.6719 - mae: 226.6920 - val_loss: 156318.9375 - val_mse: 156318.9375 - val_mae: 226.2904\n",
      "Epoch 44/100\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 156872.6406 - mse: 156872.6406 - mae: 226.5875 - val_loss: 156318.3906 - val_mse: 156318.3906 - val_mae: 226.3748\n",
      "Epoch 45/100\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 156871.2344 - mse: 156871.2344 - mae: 226.7323 - val_loss: 156326.7344 - val_mse: 156326.7344 - val_mae: 225.5134\n",
      "Epoch 46/100\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 156874.5000 - mse: 156874.5000 - mae: 226.5523 - val_loss: 156320.7812 - val_mse: 156320.7812 - val_mae: 226.0104\n",
      "Epoch 47/100\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 156871.3281 - mse: 156871.3281 - mae: 226.7030 - val_loss: 156321.4062 - val_mse: 156321.4062 - val_mae: 225.9513\n",
      "Epoch 48/100\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 156873.7812 - mse: 156873.7812 - mae: 226.5645 - val_loss: 156318.5000 - val_mse: 156318.5000 - val_mae: 226.3830\n",
      "Epoch 49/100\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 156871.1719 - mse: 156871.1719 - mae: 226.6951 - val_loss: 156321.9531 - val_mse: 156321.9531 - val_mae: 225.8697\n",
      "Epoch 50/100\n",
      "11414/11414 [==============================] - 30s 3ms/step - loss: 156873.5312 - mse: 156873.5312 - mae: 226.5670 - val_loss: 156318.0938 - val_mse: 156318.0938 - val_mae: 226.4928\n",
      "Epoch 51/100\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 156871.4844 - mse: 156871.4844 - mae: 226.5927 - val_loss: 156318.2656 - val_mse: 156318.2656 - val_mae: 226.4035\n",
      "Epoch 52/100\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 156871.6875 - mse: 156871.6875 - mae: 226.6593 - val_loss: 156319.0156 - val_mse: 156319.0156 - val_mae: 226.2235\n",
      "Epoch 53/100\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 156869.8594 - mse: 156869.8594 - mae: 226.8051 - val_loss: 156326.5312 - val_mse: 156326.5312 - val_mae: 225.5189\n",
      "Epoch 54/100\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 156871.4375 - mse: 156871.4375 - mae: 226.6680 - val_loss: 156327.5938 - val_mse: 156327.5938 - val_mae: 225.4620\n",
      "Epoch 55/100\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 156873.1406 - mse: 156873.1406 - mae: 226.6104 - val_loss: 156329.0000 - val_mse: 156329.0000 - val_mae: 225.3639\n",
      "Epoch 56/100\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 156873.8281 - mse: 156873.8281 - mae: 226.5591 - val_loss: 156319.7500 - val_mse: 156319.7500 - val_mae: 226.1265\n",
      "Epoch 57/100\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 156873.5469 - mse: 156873.5469 - mae: 226.5479 - val_loss: 156319.2500 - val_mse: 156319.2500 - val_mae: 226.1833\n",
      "Epoch 58/100\n",
      "11414/11414 [==============================] - 31s 3ms/step - loss: 156872.2031 - mse: 156872.2031 - mae: 226.6567 - val_loss: 156319.6875 - val_mse: 156319.6875 - val_mae: 226.1406\n",
      "Epoch 59/100\n",
      "11414/11414 [==============================] - 35s 3ms/step - loss: 156872.9062 - mse: 156872.9062 - mae: 226.5688 - val_loss: 156317.9531 - val_mse: 156317.9531 - val_mae: 226.6176\n",
      "Epoch 60/100\n",
      "11414/11414 [==============================] - 28s 2ms/step - loss: 156871.1562 - mse: 156871.1562 - mae: 226.7254 - val_loss: 156318.6562 - val_mse: 156318.6562 - val_mae: 226.3195\n",
      "Epoch 61/100\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 156870.8594 - mse: 156870.8594 - mae: 226.6910 - val_loss: 156318.3438 - val_mse: 156318.3438 - val_mae: 226.3602\n",
      "Epoch 62/100\n",
      "11414/11414 [==============================] - 28s 2ms/step - loss: 156873.0781 - mse: 156873.0781 - mae: 226.5874 - val_loss: 156318.3594 - val_mse: 156318.3594 - val_mae: 226.3676\n",
      "Epoch 63/100\n",
      "11414/11414 [==============================] - 28s 2ms/step - loss: 156873.3438 - mse: 156873.3438 - mae: 226.6314 - val_loss: 156318.8594 - val_mse: 156318.8594 - val_mae: 226.3021\n",
      "Epoch 64/100\n",
      "11414/11414 [==============================] - 28s 2ms/step - loss: 156873.0000 - mse: 156873.0000 - mae: 226.5905 - val_loss: 156318.9844 - val_mse: 156318.9844 - val_mae: 226.2060\n",
      "Epoch 65/100\n",
      "11414/11414 [==============================] - 28s 2ms/step - loss: 156871.7812 - mse: 156871.7812 - mae: 226.6577 - val_loss: 156318.9219 - val_mse: 156318.9219 - val_mae: 226.2973\n",
      "Epoch 66/100\n",
      "11414/11414 [==============================] - 28s 2ms/step - loss: 156872.2031 - mse: 156872.2031 - mae: 226.6636 - val_loss: 156318.1875 - val_mse: 156318.1875 - val_mae: 226.4833\n",
      "Epoch 67/100\n",
      "11414/11414 [==============================] - 28s 2ms/step - loss: 156870.0938 - mse: 156870.0938 - mae: 226.7760 - val_loss: 156329.7812 - val_mse: 156329.7812 - val_mae: 225.3297\n",
      "Epoch 68/100\n",
      "11414/11414 [==============================] - 28s 2ms/step - loss: 156871.9219 - mse: 156871.9219 - mae: 226.6900 - val_loss: 156320.0938 - val_mse: 156320.0938 - val_mae: 226.0721\n",
      "Epoch 69/100\n",
      "11414/11414 [==============================] - 28s 2ms/step - loss: 156873.2812 - mse: 156873.2812 - mae: 226.6702 - val_loss: 156321.4688 - val_mse: 156321.4688 - val_mae: 225.9120\n",
      "Epoch 70/100\n",
      "11414/11414 [==============================] - 28s 2ms/step - loss: 156873.0625 - mse: 156873.0625 - mae: 226.6566 - val_loss: 156319.4219 - val_mse: 156319.4219 - val_mae: 226.1601\n",
      "Epoch 71/100\n",
      "11414/11414 [==============================] - 28s 2ms/step - loss: 156872.6406 - mse: 156872.6406 - mae: 226.5621 - val_loss: 156317.8906 - val_mse: 156317.8906 - val_mae: 226.5888\n",
      "Epoch 72/100\n",
      "11414/11414 [==============================] - 28s 2ms/step - loss: 156871.6875 - mse: 156871.6875 - mae: 226.6762 - val_loss: 156319.0156 - val_mse: 156319.0156 - val_mae: 226.2397\n",
      "Epoch 73/100\n",
      "11414/11414 [==============================] - 28s 2ms/step - loss: 156870.3594 - mse: 156870.3594 - mae: 226.7200 - val_loss: 156331.2031 - val_mse: 156331.2031 - val_mae: 225.2512\n",
      "Epoch 74/100\n",
      "11414/11414 [==============================] - 33s 3ms/step - loss: 156875.8594 - mse: 156875.8594 - mae: 226.4560 - val_loss: 156318.5000 - val_mse: 156318.5000 - val_mae: 226.3790\n",
      "Epoch 75/100\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 156871.9688 - mse: 156871.9688 - mae: 226.5983 - val_loss: 156318.2031 - val_mse: 156318.2031 - val_mae: 226.5574\n",
      "Epoch 76/100\n",
      "11414/11414 [==============================] - 28s 2ms/step - loss: 156871.0469 - mse: 156871.0469 - mae: 226.7164 - val_loss: 156318.4688 - val_mse: 156318.4688 - val_mae: 226.3445\n",
      "Epoch 77/100\n",
      "11414/11414 [==============================] - 28s 2ms/step - loss: 156872.2812 - mse: 156872.2812 - mae: 226.5841 - val_loss: 156317.8750 - val_mse: 156317.8750 - val_mae: 226.5885\n",
      "Epoch 78/100\n",
      "11414/11414 [==============================] - 28s 2ms/step - loss: 156871.3125 - mse: 156871.3125 - mae: 226.7354 - val_loss: 156320.3906 - val_mse: 156320.3906 - val_mae: 226.0265\n",
      "Epoch 79/100\n",
      "11414/11414 [==============================] - 28s 2ms/step - loss: 156870.5781 - mse: 156870.5781 - mae: 226.7016 - val_loss: 156330.1094 - val_mse: 156330.1094 - val_mae: 225.3017\n",
      "Epoch 80/100\n",
      "11414/11414 [==============================] - 28s 2ms/step - loss: 156868.9844 - mse: 156868.9844 - mae: 226.7204 - val_loss: 156345.5938 - val_mse: 156345.5938 - val_mae: 224.6641\n",
      "Epoch 81/100\n",
      "11414/11414 [==============================] - 28s 2ms/step - loss: 156873.6250 - mse: 156873.6250 - mae: 226.5195 - val_loss: 156328.0781 - val_mse: 156328.0781 - val_mae: 225.4161\n",
      "Epoch 82/100\n",
      "11414/11414 [==============================] - 28s 2ms/step - loss: 156873.5781 - mse: 156873.5781 - mae: 226.4940 - val_loss: 156318.0312 - val_mse: 156318.0312 - val_mae: 226.5517\n",
      "Epoch 83/100\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 156868.8281 - mse: 156868.8281 - mae: 226.8625 - val_loss: 156326.3281 - val_mse: 156326.3281 - val_mae: 225.5353\n",
      "Epoch 84/100\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 156871.7812 - mse: 156871.7812 - mae: 226.6493 - val_loss: 156322.0781 - val_mse: 156322.0781 - val_mae: 225.8467\n",
      "Epoch 85/100\n",
      "11414/11414 [==============================] - 30s 3ms/step - loss: 156871.7500 - mse: 156871.7500 - mae: 226.6721 - val_loss: 156321.4062 - val_mse: 156321.4062 - val_mae: 225.9438\n",
      "Epoch 86/100\n",
      "11414/11414 [==============================] - 28s 2ms/step - loss: 156872.7344 - mse: 156872.7344 - mae: 226.6630 - val_loss: 156322.8125 - val_mse: 156322.8125 - val_mae: 225.7922\n",
      "Epoch 87/100\n",
      "11414/11414 [==============================] - 28s 2ms/step - loss: 156871.3906 - mse: 156871.3906 - mae: 226.6630 - val_loss: 156319.9688 - val_mse: 156319.9688 - val_mae: 226.0683\n",
      "Epoch 88/100\n",
      "11414/11414 [==============================] - 28s 2ms/step - loss: 156872.1875 - mse: 156872.1875 - mae: 226.6294 - val_loss: 156318.5469 - val_mse: 156318.5469 - val_mae: 226.3574\n",
      "Epoch 89/100\n",
      "11414/11414 [==============================] - 28s 2ms/step - loss: 156870.7656 - mse: 156870.7656 - mae: 226.7536 - val_loss: 156323.9531 - val_mse: 156323.9531 - val_mae: 225.7127\n",
      "Epoch 90/100\n",
      "11414/11414 [==============================] - 28s 2ms/step - loss: 156873.2656 - mse: 156873.2656 - mae: 226.6021 - val_loss: 156319.7031 - val_mse: 156319.7031 - val_mae: 226.1327\n",
      "Epoch 91/100\n",
      "11414/11414 [==============================] - 28s 2ms/step - loss: 156874.5312 - mse: 156874.5312 - mae: 226.4785 - val_loss: 156317.7969 - val_mse: 156317.7969 - val_mae: 226.7413\n",
      "Epoch 92/100\n",
      "11414/11414 [==============================] - 28s 2ms/step - loss: 156870.9375 - mse: 156870.9375 - mae: 226.7748 - val_loss: 156321.8281 - val_mse: 156321.8281 - val_mae: 225.8709\n",
      "Epoch 93/100\n",
      "11414/11414 [==============================] - 28s 2ms/step - loss: 156873.1250 - mse: 156873.1250 - mae: 226.6294 - val_loss: 156318.2812 - val_mse: 156318.2812 - val_mae: 226.4454\n",
      "Epoch 94/100\n",
      "11414/11414 [==============================] - 29s 2ms/step - loss: 156872.2500 - mse: 156872.2500 - mae: 226.6774 - val_loss: 156321.3594 - val_mse: 156321.3594 - val_mae: 225.9370\n",
      "Epoch 95/100\n",
      "11414/11414 [==============================] - 28s 2ms/step - loss: 156872.9531 - mse: 156872.9531 - mae: 226.5719 - val_loss: 156318.2344 - val_mse: 156318.2344 - val_mae: 226.7623\n",
      "Epoch 96/100\n",
      "11414/11414 [==============================] - 28s 2ms/step - loss: 156872.6875 - mse: 156872.6875 - mae: 226.7021 - val_loss: 156318.0625 - val_mse: 156318.0625 - val_mae: 226.5978\n",
      "Epoch 97/100\n",
      "11414/11414 [==============================] - 28s 2ms/step - loss: 156872.4844 - mse: 156872.4844 - mae: 226.6831 - val_loss: 156320.0781 - val_mse: 156320.0781 - val_mae: 226.0712\n",
      "Epoch 98/100\n",
      "11414/11414 [==============================] - 31s 3ms/step - loss: 156873.0312 - mse: 156873.0312 - mae: 226.6271 - val_loss: 156320.4375 - val_mse: 156320.4375 - val_mae: 226.0205\n",
      "Epoch 99/100\n",
      "11414/11414 [==============================] - 31s 3ms/step - loss: 156873.0000 - mse: 156873.0000 - mae: 226.6680 - val_loss: 156318.4375 - val_mse: 156318.4375 - val_mae: 226.4274\n",
      "Epoch 100/100\n",
      "11414/11414 [==============================] - 32s 3ms/step - loss: 156871.8906 - mse: 156871.8906 - mae: 226.7017 - val_loss: 156318.5000 - val_mse: 156318.5000 - val_mae: 226.3901\n"
     ]
    }
   ],
   "source": [
    "# 학습 실시 (hist = )\n",
    "hist = model_dl.fit(train_x, train_y, epochs = 100, validation_data = (test_x, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a7c19767",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, loss_ax = plt.subplots()\n",
    "\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(hist.history['val_loss'], 'r', label='test loss')\n",
    "\n",
    "acc_ax.plot(hist.history['mae'], 'b', label='train mae')\n",
    "acc_ax.plot(hist.history['val_mae'], 'g', label='val mae')\n",
    "\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "acc_ax.set_ylabel('mae')\n",
    "\n",
    "loss_ax.legend(loc='upper left')\n",
    "acc_ax.legend(loc='lower left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a9927723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11398/11414 [============================>.] - ETA: 0s - loss: 156922.4531 - mse: 156922.4531 - mae: 226.7070\n",
      "Epoch 1: val_loss improved from inf to 156325.81250, saving model to my_checkpoint.ckpt\n",
      "11414/11414 [==============================] - 31s 3ms/step - loss: 156871.7188 - mse: 156871.7188 - mae: 226.7052 - val_loss: 156325.8125 - val_mse: 156325.8125 - val_mae: 225.5660\n",
      "Epoch 2/100\n",
      "11398/11414 [============================>.] - ETA: 0s - loss: 156936.3750 - mse: 156936.3750 - mae: 226.7079\n",
      "Epoch 2: val_loss improved from 156325.81250 to 156325.79688, saving model to my_checkpoint.ckpt\n",
      "11414/11414 [==============================] - 31s 3ms/step - loss: 156872.2500 - mse: 156872.2500 - mae: 226.6731 - val_loss: 156325.7969 - val_mse: 156325.7969 - val_mae: 225.5646\n",
      "Epoch 3/100\n",
      "11409/11414 [============================>.] - ETA: 0s - loss: 156877.2969 - mse: 156877.2969 - mae: 226.6946\n",
      "Epoch 3: val_loss did not improve from 156325.79688\n",
      "11414/11414 [==============================] - 31s 3ms/step - loss: 156871.7656 - mse: 156871.7656 - mae: 226.6894 - val_loss: 156326.6406 - val_mse: 156326.6406 - val_mae: 225.5052\n",
      "Epoch 4/100\n",
      "11402/11414 [============================>.] - ETA: 0s - loss: 156926.9219 - mse: 156926.9219 - mae: 226.6045\n",
      "Epoch 4: val_loss improved from 156325.79688 to 156324.00000, saving model to my_checkpoint.ckpt\n",
      "11414/11414 [==============================] - 30s 3ms/step - loss: 156874.0156 - mse: 156874.0156 - mae: 226.5953 - val_loss: 156324.0000 - val_mse: 156324.0000 - val_mae: 225.6881\n",
      "Epoch 5/100\n",
      "11404/11414 [============================>.] - ETA: 0s - loss: 156958.7812 - mse: 156958.7812 - mae: 226.5724\n",
      "Epoch 5: val_loss improved from 156324.00000 to 156319.06250, saving model to my_checkpoint.ckpt\n",
      "11414/11414 [==============================] - 34s 3ms/step - loss: 156874.2812 - mse: 156874.2812 - mae: 226.5361 - val_loss: 156319.0625 - val_mse: 156319.0625 - val_mae: 226.2276\n",
      "Epoch 6/100\n",
      "11410/11414 [============================>.] - ETA: 0s - loss: 156869.5938 - mse: 156869.5938 - mae: 226.6489\n",
      "Epoch 6: val_loss did not improve from 156319.06250\n",
      "11414/11414 [==============================] - 32s 3ms/step - loss: 156872.3906 - mse: 156872.3906 - mae: 226.6560 - val_loss: 156320.2031 - val_mse: 156320.2031 - val_mae: 226.0385\n",
      "Epoch 7/100\n",
      "11398/11414 [============================>.] - ETA: 0s - loss: 156939.3125 - mse: 156939.3125 - mae: 226.6478\n",
      "Epoch 7: val_loss did not improve from 156319.06250\n",
      "11414/11414 [==============================] - 30s 3ms/step - loss: 156873.0625 - mse: 156873.0625 - mae: 226.6344 - val_loss: 156322.5156 - val_mse: 156322.5156 - val_mae: 225.8288\n",
      "Epoch 8/100\n",
      "11413/11414 [============================>.] - ETA: 0s - loss: 156878.5312 - mse: 156878.5312 - mae: 226.6366\n",
      "Epoch 8: val_loss improved from 156319.06250 to 156318.54688, saving model to my_checkpoint.ckpt\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 156872.5000 - mse: 156872.5000 - mae: 226.6351 - val_loss: 156318.5469 - val_mse: 156318.5469 - val_mae: 226.3388\n",
      "Epoch 9/100\n",
      "11412/11414 [============================>.] - ETA: 0s - loss: 156883.4219 - mse: 156883.4219 - mae: 226.6868\n",
      "Epoch 9: val_loss did not improve from 156318.54688\n",
      "11414/11414 [==============================] - 31s 3ms/step - loss: 156871.7500 - mse: 156871.7500 - mae: 226.6827 - val_loss: 156321.0938 - val_mse: 156321.0938 - val_mae: 225.9623\n",
      "Epoch 10/100\n",
      "11406/11414 [============================>.] - ETA: 0s - loss: 156860.0312 - mse: 156860.0312 - mae: 226.6810\n",
      "Epoch 10: val_loss did not improve from 156318.54688\n",
      "11414/11414 [==============================] - 34s 3ms/step - loss: 156872.0781 - mse: 156872.0781 - mae: 226.6941 - val_loss: 156322.6406 - val_mse: 156322.6406 - val_mae: 225.8123\n",
      "Epoch 11/100\n",
      "11394/11414 [============================>.] - ETA: 0s - loss: 156914.8125 - mse: 156914.8125 - mae: 226.6234\n",
      "Epoch 11: val_loss improved from 156318.54688 to 156318.37500, saving model to my_checkpoint.ckpt\n",
      "11414/11414 [==============================] - 33s 3ms/step - loss: 156872.5469 - mse: 156872.5469 - mae: 226.6186 - val_loss: 156318.3750 - val_mse: 156318.3750 - val_mae: 226.4390\n",
      "Epoch 12/100\n",
      "11414/11414 [==============================] - ETA: 0s - loss: 156872.1406 - mse: 156872.1406 - mae: 226.6649\n",
      "Epoch 12: val_loss did not improve from 156318.37500\n",
      "11414/11414 [==============================] - 33s 3ms/step - loss: 156872.1406 - mse: 156872.1406 - mae: 226.6649 - val_loss: 156320.8750 - val_mse: 156320.8750 - val_mae: 226.0069\n",
      "Epoch 13/100\n",
      "11411/11414 [============================>.] - ETA: 0s - loss: 156882.5156 - mse: 156882.5156 - mae: 226.6233\n",
      "Epoch 13: val_loss did not improve from 156318.37500\n",
      "11414/11414 [==============================] - 32s 3ms/step - loss: 156872.5625 - mse: 156872.5625 - mae: 226.6229 - val_loss: 156320.1719 - val_mse: 156320.1719 - val_mae: 226.0362\n",
      "Epoch 14/100\n",
      "11413/11414 [============================>.] - ETA: 0s - loss: 156879.5781 - mse: 156879.5781 - mae: 226.7059\n",
      "Epoch 14: val_loss did not improve from 156318.37500\n",
      "11414/11414 [==============================] - 39s 3ms/step - loss: 156872.0312 - mse: 156872.0312 - mae: 226.7020 - val_loss: 156320.3281 - val_mse: 156320.3281 - val_mae: 226.0316\n",
      "Epoch 15/100\n",
      "11394/11414 [============================>.] - ETA: 0s - loss: 156869.2500 - mse: 156869.2500 - mae: 226.7302\n",
      "Epoch 15: val_loss did not improve from 156318.37500\n",
      "11414/11414 [==============================] - 34s 3ms/step - loss: 156870.3281 - mse: 156870.3281 - mae: 226.7596 - val_loss: 156321.7188 - val_mse: 156321.7188 - val_mae: 225.8717\n",
      "Epoch 16/100\n",
      "11400/11414 [============================>.] - ETA: 0s - loss: 156887.8750 - mse: 156887.8750 - mae: 226.6973\n",
      "Epoch 16: val_loss did not improve from 156318.37500\n",
      "11414/11414 [==============================] - 34s 3ms/step - loss: 156871.0625 - mse: 156871.0625 - mae: 226.6951 - val_loss: 156319.4219 - val_mse: 156319.4219 - val_mae: 226.1648\n",
      "Epoch 17/100\n",
      "11405/11414 [============================>.] - ETA: 0s - loss: 156921.9219 - mse: 156921.9219 - mae: 226.7263\n",
      "Epoch 17: val_loss did not improve from 156318.37500\n",
      "11414/11414 [==============================] - 33s 3ms/step - loss: 156871.3438 - mse: 156871.3438 - mae: 226.7127 - val_loss: 156320.0781 - val_mse: 156320.0781 - val_mae: 226.0474\n",
      "Epoch 18/100\n",
      "11396/11414 [============================>.] - ETA: 0s - loss: 156990.0156 - mse: 156990.0156 - mae: 226.6285\n",
      "Epoch 18: val_loss did not improve from 156318.37500\n",
      "11414/11414 [==============================] - 32s 3ms/step - loss: 156873.0156 - mse: 156873.0156 - mae: 226.6006 - val_loss: 156319.0156 - val_mse: 156319.0156 - val_mae: 226.2308\n",
      "Epoch 19/100\n",
      "11408/11414 [============================>.] - ETA: 0s - loss: 156893.4844 - mse: 156893.4844 - mae: 226.6642\n",
      "Epoch 19: val_loss did not improve from 156318.37500\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 156872.4062 - mse: 156872.4062 - mae: 226.6681 - val_loss: 156318.8594 - val_mse: 156318.8594 - val_mae: 226.3080\n",
      "Epoch 20/100\n",
      "11401/11414 [============================>.] - ETA: 0s - loss: 156913.4219 - mse: 156913.4219 - mae: 226.6448\n",
      "Epoch 20: val_loss improved from 156318.37500 to 156317.95312, saving model to my_checkpoint.ckpt\n",
      "11414/11414 [==============================] - 31s 3ms/step - loss: 156872.7500 - mse: 156872.7500 - mae: 226.6256 - val_loss: 156317.9531 - val_mse: 156317.9531 - val_mae: 226.5033\n",
      "Epoch 21/100\n",
      "11408/11414 [============================>.] - ETA: 0s - loss: 156854.0312 - mse: 156854.0312 - mae: 226.6638\n",
      "Epoch 21: val_loss did not improve from 156317.95312\n",
      "11414/11414 [==============================] - 31s 3ms/step - loss: 156871.9375 - mse: 156871.9375 - mae: 226.6819 - val_loss: 156321.7500 - val_mse: 156321.7500 - val_mae: 225.8923\n",
      "Epoch 22/100\n",
      "11393/11414 [============================>.] - ETA: 0s - loss: 156968.1719 - mse: 156968.1719 - mae: 226.7230\n",
      "Epoch 22: val_loss did not improve from 156317.95312\n",
      "11414/11414 [==============================] - 31s 3ms/step - loss: 156871.1094 - mse: 156871.1094 - mae: 226.7071 - val_loss: 156320.3906 - val_mse: 156320.3906 - val_mae: 226.0265\n",
      "Epoch 23/100\n",
      "11401/11414 [============================>.] - ETA: 0s - loss: 156909.6406 - mse: 156909.6406 - mae: 226.6565\n",
      "Epoch 23: val_loss did not improve from 156317.95312\n",
      "11414/11414 [==============================] - 31s 3ms/step - loss: 156872.7969 - mse: 156872.7969 - mae: 226.6557 - val_loss: 156321.4375 - val_mse: 156321.4375 - val_mae: 225.9130\n",
      "Epoch 24/100\n",
      "11405/11414 [============================>.] - ETA: 0s - loss: 156846.8281 - mse: 156846.8281 - mae: 226.6104\n",
      "Epoch 24: val_loss did not improve from 156317.95312\n",
      "11414/11414 [==============================] - 31s 3ms/step - loss: 156871.8281 - mse: 156871.8281 - mae: 226.6253 - val_loss: 156318.2188 - val_mse: 156318.2188 - val_mae: 226.5574\n",
      "Epoch 25/100\n",
      "11397/11414 [============================>.] - ETA: 0s - loss: 156763.0312 - mse: 156763.0312 - mae: 226.6397\n",
      "Epoch 25: val_loss did not improve from 156317.95312\n",
      "11414/11414 [==============================] - 32s 3ms/step - loss: 156872.2031 - mse: 156872.2031 - mae: 226.6537 - val_loss: 156320.0469 - val_mse: 156320.0469 - val_mae: 226.0835\n",
      "Epoch 26/100\n",
      "11403/11414 [============================>.] - ETA: 0s - loss: 156942.0156 - mse: 156942.0156 - mae: 226.6794\n",
      "Epoch 26: val_loss did not improve from 156317.95312\n",
      "11414/11414 [==============================] - 33s 3ms/step - loss: 156872.6875 - mse: 156872.6875 - mae: 226.6525 - val_loss: 156321.2031 - val_mse: 156321.2031 - val_mae: 225.9557\n",
      "Epoch 27/100\n",
      "11403/11414 [============================>.] - ETA: 0s - loss: 156845.7188 - mse: 156845.7188 - mae: 226.7361\n",
      "Epoch 27: val_loss did not improve from 156317.95312\n",
      "11414/11414 [==============================] - 31s 3ms/step - loss: 156870.2969 - mse: 156870.2969 - mae: 226.7457 - val_loss: 156329.7812 - val_mse: 156329.7812 - val_mae: 225.3259\n",
      "Epoch 28/100\n",
      "11404/11414 [============================>.] - ETA: 0s - loss: 156899.4844 - mse: 156899.4844 - mae: 226.5997\n",
      "Epoch 28: val_loss did not improve from 156317.95312\n",
      "11414/11414 [==============================] - 31s 3ms/step - loss: 156873.2969 - mse: 156873.2969 - mae: 226.5980 - val_loss: 156320.7500 - val_mse: 156320.7500 - val_mae: 226.0121\n",
      "Epoch 29/100\n",
      "11393/11414 [============================>.] - ETA: 0s - loss: 156836.9844 - mse: 156836.9844 - mae: 226.6929\n",
      "Epoch 29: val_loss did not improve from 156317.95312\n",
      "11414/11414 [==============================] - 31s 3ms/step - loss: 156872.0938 - mse: 156872.0938 - mae: 226.7139 - val_loss: 156321.7188 - val_mse: 156321.7188 - val_mae: 225.9001\n",
      "Epoch 30/100\n",
      "11407/11414 [============================>.] - ETA: 0s - loss: 156907.7500 - mse: 156907.7500 - mae: 226.6080\n",
      "Epoch 30: val_loss did not improve from 156317.95312\n",
      "11414/11414 [==============================] - 31s 3ms/step - loss: 156872.0781 - mse: 156872.0781 - mae: 226.5944 - val_loss: 156317.9844 - val_mse: 156317.9844 - val_mae: 226.6349\n",
      "Epoch 31/100\n",
      "11392/11414 [============================>.] - ETA: 0s - loss: 156914.4844 - mse: 156914.4844 - mae: 226.7644\n",
      "Epoch 31: val_loss did not improve from 156317.95312\n",
      "11414/11414 [==============================] - 31s 3ms/step - loss: 156870.5781 - mse: 156870.5781 - mae: 226.7543 - val_loss: 156323.9062 - val_mse: 156323.9062 - val_mae: 225.6945\n",
      "Epoch 32/100\n",
      "11399/11414 [============================>.] - ETA: 0s - loss: 156912.5781 - mse: 156912.5781 - mae: 226.7675\n",
      "Epoch 32: val_loss did not improve from 156317.95312\n",
      "11414/11414 [==============================] - 28s 2ms/step - loss: 156871.2969 - mse: 156871.2969 - mae: 226.7472 - val_loss: 156326.6719 - val_mse: 156326.6719 - val_mae: 225.5133\n",
      "Epoch 33/100\n",
      "11405/11414 [============================>.] - ETA: 0s - loss: 156898.4531 - mse: 156898.4531 - mae: 226.5038\n",
      "Epoch 33: val_loss did not improve from 156317.95312\n",
      "11414/11414 [==============================] - 28s 2ms/step - loss: 156872.7344 - mse: 156872.7344 - mae: 226.5065 - val_loss: 156318.3906 - val_mse: 156318.3906 - val_mae: 226.4305\n",
      "Epoch 34/100\n",
      "11404/11414 [============================>.] - ETA: 0s - loss: 156913.7188 - mse: 156913.7188 - mae: 226.7775\n",
      "Epoch 34: val_loss did not improve from 156317.95312\n",
      "11414/11414 [==============================] - 28s 2ms/step - loss: 156870.5781 - mse: 156870.5781 - mae: 226.7711 - val_loss: 156325.4219 - val_mse: 156325.4219 - val_mae: 225.5999\n",
      "Epoch 35/100\n",
      "11397/11414 [============================>.] - ETA: 0s - loss: 156991.4688 - mse: 156991.4688 - mae: 226.7111\n",
      "Epoch 35: val_loss did not improve from 156317.95312\n",
      "11414/11414 [==============================] - 28s 2ms/step - loss: 156872.1250 - mse: 156872.1250 - mae: 226.6704 - val_loss: 156325.9375 - val_mse: 156325.9375 - val_mae: 225.5562\n",
      "Epoch 36/100\n",
      "11411/11414 [============================>.] - ETA: 0s - loss: 156894.0156 - mse: 156894.0156 - mae: 226.5607\n",
      "Epoch 36: val_loss did not improve from 156317.95312\n",
      "11414/11414 [==============================] - 28s 2ms/step - loss: 156874.0156 - mse: 156874.0156 - mae: 226.5544 - val_loss: 156322.1250 - val_mse: 156322.1250 - val_mae: 225.8514\n",
      "Epoch 37/100\n",
      "11404/11414 [============================>.] - ETA: 0s - loss: 156876.8125 - mse: 156876.8125 - mae: 226.7429\n",
      "Epoch 37: val_loss did not improve from 156317.95312\n",
      "11414/11414 [==============================] - 28s 2ms/step - loss: 156869.7344 - mse: 156869.7344 - mae: 226.7496 - val_loss: 156330.8594 - val_mse: 156330.8594 - val_mae: 225.2749\n",
      "Epoch 38/100\n",
      "11409/11414 [============================>.] - ETA: 0s - loss: 156890.5312 - mse: 156890.5312 - mae: 226.5595\n",
      "Epoch 38: val_loss did not improve from 156317.95312\n",
      "11414/11414 [==============================] - 28s 2ms/step - loss: 156872.9531 - mse: 156872.9531 - mae: 226.5564 - val_loss: 156326.6719 - val_mse: 156326.6719 - val_mae: 225.5116\n",
      "Epoch 39/100\n",
      "11413/11414 [============================>.] - ETA: 0s - loss: 156880.0156 - mse: 156880.0156 - mae: 226.6168\n",
      "Epoch 39: val_loss did not improve from 156317.95312\n",
      "11414/11414 [==============================] - 28s 2ms/step - loss: 156872.7188 - mse: 156872.7188 - mae: 226.6136 - val_loss: 156319.7500 - val_mse: 156319.7500 - val_mae: 226.1235\n",
      "Epoch 40/100\n",
      "11408/11414 [============================>.] - ETA: 0s - loss: 156921.0156 - mse: 156921.0156 - mae: 226.6509\n",
      "Epoch 40: val_loss did not improve from 156317.95312\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 156872.9688 - mse: 156872.9688 - mae: 226.6330 - val_loss: 156319.8750 - val_mse: 156319.8750 - val_mae: 226.0934\n",
      "Epoch 41/100\n",
      "11401/11414 [============================>.] - ETA: 0s - loss: 156887.1250 - mse: 156887.1250 - mae: 226.6140\n",
      "Epoch 41: val_loss did not improve from 156317.95312\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 156872.8125 - mse: 156872.8125 - mae: 226.6051 - val_loss: 156319.7344 - val_mse: 156319.7344 - val_mae: 226.1394\n",
      "Epoch 42/100\n",
      "11408/11414 [============================>.] - ETA: 0s - loss: 156809.3281 - mse: 156809.3281 - mae: 226.6814\n",
      "Epoch 42: val_loss did not improve from 156317.95312\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 156869.5000 - mse: 156869.5000 - mae: 226.7094 - val_loss: 156331.5625 - val_mse: 156331.5625 - val_mae: 225.2391\n",
      "Epoch 43/100\n",
      "11397/11414 [============================>.] - ETA: 0s - loss: 156826.3750 - mse: 156826.3750 - mae: 226.3820\n",
      "Epoch 43: val_loss improved from 156317.95312 to 156317.92188, saving model to my_checkpoint.ckpt\n",
      "11414/11414 [==============================] - 28s 2ms/step - loss: 156875.1875 - mse: 156875.1875 - mae: 226.4090 - val_loss: 156317.9219 - val_mse: 156317.9219 - val_mae: 226.6436\n",
      "Epoch 44/100\n",
      "11414/11414 [==============================] - ETA: 0s - loss: 156871.8594 - mse: 156871.8594 - mae: 226.7546\n",
      "Epoch 44: val_loss did not improve from 156317.92188\n",
      "11414/11414 [==============================] - 28s 2ms/step - loss: 156871.8594 - mse: 156871.8594 - mae: 226.7546 - val_loss: 156319.7188 - val_mse: 156319.7188 - val_mae: 226.1303\n",
      "Epoch 45/100\n",
      "11412/11414 [============================>.] - ETA: 0s - loss: 156865.6250 - mse: 156865.6250 - mae: 226.5617\n",
      "Epoch 45: val_loss did not improve from 156317.92188\n",
      "11414/11414 [==============================] - 28s 2ms/step - loss: 156872.8281 - mse: 156872.8281 - mae: 226.5713 - val_loss: 156318.2969 - val_mse: 156318.2969 - val_mae: 226.4038\n",
      "Epoch 46/100\n",
      "11394/11414 [============================>.] - ETA: 0s - loss: 156941.2188 - mse: 156941.2188 - mae: 226.7360\n",
      "Epoch 46: val_loss did not improve from 156317.92188\n",
      "11414/11414 [==============================] - 28s 2ms/step - loss: 156871.1406 - mse: 156871.1406 - mae: 226.7279 - val_loss: 156322.9531 - val_mse: 156322.9531 - val_mae: 225.7734\n",
      "Epoch 47/100\n",
      "11393/11414 [============================>.] - ETA: 0s - loss: 156897.2344 - mse: 156897.2344 - mae: 226.7048\n",
      "Epoch 47: val_loss did not improve from 156317.92188\n",
      "11414/11414 [==============================] - 31s 3ms/step - loss: 156872.1875 - mse: 156872.1875 - mae: 226.6856 - val_loss: 156326.0156 - val_mse: 156326.0156 - val_mae: 225.5491\n",
      "Epoch 48/100\n",
      "11408/11414 [============================>.] - ETA: 0s - loss: 156864.6719 - mse: 156864.6719 - mae: 226.7481\n",
      "Epoch 48: val_loss did not improve from 156317.92188\n",
      "11414/11414 [==============================] - 31s 3ms/step - loss: 156870.9062 - mse: 156870.9062 - mae: 226.7472 - val_loss: 156322.6250 - val_mse: 156322.6250 - val_mae: 225.8158\n",
      "Epoch 49/100\n",
      "11414/11414 [==============================] - ETA: 0s - loss: 156872.3594 - mse: 156872.3594 - mae: 226.6394\n",
      "Epoch 49: val_loss did not improve from 156317.92188\n",
      "11414/11414 [==============================] - 32s 3ms/step - loss: 156872.3594 - mse: 156872.3594 - mae: 226.6394 - val_loss: 156318.5312 - val_mse: 156318.5312 - val_mae: 226.3572\n",
      "Epoch 50/100\n",
      "11412/11414 [============================>.] - ETA: 0s - loss: 156884.0000 - mse: 156884.0000 - mae: 226.7406\n",
      "Epoch 50: val_loss did not improve from 156317.92188\n",
      "11414/11414 [==============================] - 30s 3ms/step - loss: 156871.2812 - mse: 156871.2812 - mae: 226.7378 - val_loss: 156322.0312 - val_mse: 156322.0312 - val_mae: 225.8594\n",
      "Epoch 51/100\n",
      "11410/11414 [============================>.] - ETA: 0s - loss: 156896.2031 - mse: 156896.2031 - mae: 226.6419\n",
      "Epoch 51: val_loss did not improve from 156317.92188\n",
      "11414/11414 [==============================] - 31s 3ms/step - loss: 156873.0000 - mse: 156873.0000 - mae: 226.6354 - val_loss: 156320.0469 - val_mse: 156320.0469 - val_mae: 226.0509\n",
      "Epoch 52/100\n",
      "11411/11414 [============================>.] - ETA: 0s - loss: 156892.3906 - mse: 156892.3906 - mae: 226.6401\n",
      "Epoch 52: val_loss did not improve from 156317.92188\n",
      "11414/11414 [==============================] - 32s 3ms/step - loss: 156872.4219 - mse: 156872.4219 - mae: 226.6357 - val_loss: 156319.3281 - val_mse: 156319.3281 - val_mae: 226.1680\n",
      "Epoch 53/100\n",
      "11398/11414 [============================>.] - ETA: 0s - loss: 156901.5000 - mse: 156901.5000 - mae: 226.6894\n",
      "Epoch 53: val_loss did not improve from 156317.92188\n",
      "11414/11414 [==============================] - 33s 3ms/step - loss: 156872.7969 - mse: 156872.7969 - mae: 226.6832 - val_loss: 156320.9219 - val_mse: 156320.9219 - val_mae: 225.9759\n",
      "Epoch 54/100\n",
      "11403/11414 [============================>.] - ETA: 0s - loss: 156878.7031 - mse: 156878.7031 - mae: 226.6877\n",
      "Epoch 54: val_loss did not improve from 156317.92188\n",
      "11414/11414 [==============================] - 33s 3ms/step - loss: 156870.5781 - mse: 156870.5781 - mae: 226.6879 - val_loss: 156319.8125 - val_mse: 156319.8125 - val_mae: 226.0989\n",
      "Epoch 55/100\n",
      "11408/11414 [============================>.] - ETA: 0s - loss: 156898.8438 - mse: 156898.8438 - mae: 226.6760\n",
      "Epoch 55: val_loss did not improve from 156317.92188\n",
      "11414/11414 [==============================] - 32s 3ms/step - loss: 156872.4375 - mse: 156872.4375 - mae: 226.6690 - val_loss: 156319.9219 - val_mse: 156319.9219 - val_mae: 226.0627\n",
      "Epoch 56/100\n",
      "11395/11414 [============================>.] - ETA: 0s - loss: 156896.2500 - mse: 156896.2500 - mae: 226.6116\n",
      "Epoch 56: val_loss did not improve from 156317.92188\n",
      "11414/11414 [==============================] - 30s 3ms/step - loss: 156872.6406 - mse: 156872.6406 - mae: 226.6253 - val_loss: 156319.0469 - val_mse: 156319.0469 - val_mae: 226.2270\n",
      "Epoch 57/100\n",
      "11414/11414 [==============================] - ETA: 0s - loss: 156871.5312 - mse: 156871.5312 - mae: 226.7273\n",
      "Epoch 57: val_loss did not improve from 156317.92188\n",
      "11414/11414 [==============================] - 33s 3ms/step - loss: 156871.5312 - mse: 156871.5312 - mae: 226.7273 - val_loss: 156320.1250 - val_mse: 156320.1250 - val_mae: 226.0719\n",
      "Epoch 58/100\n",
      "11406/11414 [============================>.] - ETA: 0s - loss: 156866.0938 - mse: 156866.0938 - mae: 226.7319\n",
      "Epoch 58: val_loss did not improve from 156317.92188\n",
      "11414/11414 [==============================] - 32s 3ms/step - loss: 156870.7656 - mse: 156870.7656 - mae: 226.7270 - val_loss: 156331.0625 - val_mse: 156331.0625 - val_mae: 225.2536\n",
      "Epoch 59/100\n",
      "11398/11414 [============================>.] - ETA: 0s - loss: 156937.6562 - mse: 156937.6562 - mae: 226.5320\n",
      "Epoch 59: val_loss did not improve from 156317.92188\n",
      "11414/11414 [==============================] - 32s 3ms/step - loss: 156873.2812 - mse: 156873.2812 - mae: 226.5173 - val_loss: 156322.2031 - val_mse: 156322.2031 - val_mae: 225.8565\n",
      "Epoch 60/100\n",
      "11400/11414 [============================>.] - ETA: 0s - loss: 156874.6406 - mse: 156874.6406 - mae: 226.7562\n",
      "Epoch 60: val_loss did not improve from 156317.92188\n",
      "11414/11414 [==============================] - 39s 3ms/step - loss: 156870.0625 - mse: 156870.0625 - mae: 226.7544 - val_loss: 156331.8594 - val_mse: 156331.8594 - val_mae: 225.2194\n",
      "Epoch 61/100\n",
      "11409/11414 [============================>.] - ETA: 0s - loss: 156893.0938 - mse: 156893.0938 - mae: 226.5180\n",
      "Epoch 61: val_loss did not improve from 156317.92188\n",
      "11414/11414 [==============================] - 36s 3ms/step - loss: 156874.1562 - mse: 156874.1562 - mae: 226.5155 - val_loss: 156322.3594 - val_mse: 156322.3594 - val_mae: 225.8367\n",
      "Epoch 62/100\n",
      "11403/11414 [============================>.] - ETA: 0s - loss: 156920.4531 - mse: 156920.4531 - mae: 226.6591\n",
      "Epoch 62: val_loss did not improve from 156317.92188\n",
      "11414/11414 [==============================] - 34s 3ms/step - loss: 156873.0000 - mse: 156873.0000 - mae: 226.6440 - val_loss: 156321.3438 - val_mse: 156321.3438 - val_mae: 225.9478\n",
      "Epoch 63/100\n",
      "11405/11414 [============================>.] - ETA: 0s - loss: 156888.3438 - mse: 156888.3438 - mae: 226.6707\n",
      "Epoch 63: val_loss did not improve from 156317.92188\n",
      "11414/11414 [==============================] - 33s 3ms/step - loss: 156871.5312 - mse: 156871.5312 - mae: 226.6808 - val_loss: 156319.8281 - val_mse: 156319.8281 - val_mae: 226.0989\n",
      "Epoch 64/100\n",
      "11412/11414 [============================>.] - ETA: 0s - loss: 156886.0781 - mse: 156886.0781 - mae: 226.6390\n",
      "Epoch 64: val_loss did not improve from 156317.92188\n",
      "11414/11414 [==============================] - 51s 4ms/step - loss: 156872.3906 - mse: 156872.3906 - mae: 226.6353 - val_loss: 156319.0469 - val_mse: 156319.0469 - val_mae: 226.2266\n",
      "Epoch 65/100\n",
      "11414/11414 [==============================] - ETA: 0s - loss: 156871.4531 - mse: 156871.4531 - mae: 226.7068\n",
      "Epoch 65: val_loss did not improve from 156317.92188\n",
      "11414/11414 [==============================] - 49s 4ms/step - loss: 156871.4531 - mse: 156871.4531 - mae: 226.7068 - val_loss: 156319.8281 - val_mse: 156319.8281 - val_mae: 226.0990\n",
      "Epoch 66/100\n",
      "11404/11414 [============================>.] - ETA: 0s - loss: 156898.6562 - mse: 156898.6562 - mae: 226.6919\n",
      "Epoch 66: val_loss did not improve from 156317.92188\n",
      "11414/11414 [==============================] - 36s 3ms/step - loss: 156871.8906 - mse: 156871.8906 - mae: 226.6943 - val_loss: 156323.1250 - val_mse: 156323.1250 - val_mae: 225.7608\n",
      "Epoch 67/100\n",
      "11391/11414 [============================>.] - ETA: 0s - loss: 157017.5312 - mse: 157017.5312 - mae: 226.7469\n",
      "Epoch 67: val_loss did not improve from 156317.92188\n",
      "11414/11414 [==============================] - 31s 3ms/step - loss: 156871.4062 - mse: 156871.4062 - mae: 226.7007 - val_loss: 156325.5781 - val_mse: 156325.5781 - val_mae: 225.5819\n",
      "Epoch 68/100\n",
      "11402/11414 [============================>.] - ETA: 0s - loss: 156941.7188 - mse: 156941.7188 - mae: 226.5312\n",
      "Epoch 68: val_loss did not improve from 156317.92188\n",
      "11414/11414 [==============================] - 35s 3ms/step - loss: 156872.7344 - mse: 156872.7344 - mae: 226.5165 - val_loss: 156318.2656 - val_mse: 156318.2656 - val_mae: 226.3614\n",
      "Epoch 69/100\n",
      "11408/11414 [============================>.] - ETA: 0s - loss: 156893.4531 - mse: 156893.4531 - mae: 226.6642\n",
      "Epoch 69: val_loss did not improve from 156317.92188\n",
      "11414/11414 [==============================] - 35s 3ms/step - loss: 156872.5938 - mse: 156872.5938 - mae: 226.6472 - val_loss: 156319.9375 - val_mse: 156319.9375 - val_mae: 226.1057\n",
      "Epoch 70/100\n",
      "11398/11414 [============================>.] - ETA: 0s - loss: 156950.5938 - mse: 156950.5938 - mae: 226.5723\n",
      "Epoch 70: val_loss did not improve from 156317.92188\n",
      "11414/11414 [==============================] - 36s 3ms/step - loss: 156874.2344 - mse: 156874.2344 - mae: 226.5460 - val_loss: 156318.9375 - val_mse: 156318.9375 - val_mae: 226.2536\n",
      "Epoch 71/100\n",
      "11402/11414 [============================>.] - ETA: 0s - loss: 156897.0156 - mse: 156897.0156 - mae: 226.7186\n",
      "Epoch 71: val_loss did not improve from 156317.92188\n",
      "11414/11414 [==============================] - 30s 3ms/step - loss: 156871.3125 - mse: 156871.3125 - mae: 226.7163 - val_loss: 156325.0000 - val_mse: 156325.0000 - val_mae: 225.6214\n",
      "Epoch 72/100\n",
      "11398/11414 [============================>.] - ETA: 0s - loss: 156884.7031 - mse: 156884.7031 - mae: 226.4908\n",
      "Epoch 72: val_loss did not improve from 156317.92188\n",
      "11414/11414 [==============================] - 32s 3ms/step - loss: 156874.5625 - mse: 156874.5625 - mae: 226.4947 - val_loss: 156318.9531 - val_mse: 156318.9531 - val_mae: 226.2867\n",
      "Epoch 73/100\n",
      "11411/11414 [============================>.] - ETA: 0s - loss: 156865.9688 - mse: 156865.9688 - mae: 226.6235\n",
      "Epoch 73: val_loss did not improve from 156317.92188\n",
      "11414/11414 [==============================] - 33s 3ms/step - loss: 156873.3906 - mse: 156873.3906 - mae: 226.6263 - val_loss: 156318.3750 - val_mse: 156318.3750 - val_mae: 226.3715\n",
      "Epoch 74/100\n",
      "11404/11414 [============================>.] - ETA: 0s - loss: 156921.6250 - mse: 156921.6250 - mae: 226.5053\n",
      "Epoch 74: val_loss did not improve from 156317.92188\n",
      "11414/11414 [==============================] - 36s 3ms/step - loss: 156872.0781 - mse: 156872.0781 - mae: 226.5024 - val_loss: 156318.2500 - val_mse: 156318.2500 - val_mae: 226.8349\n",
      "Epoch 75/100\n",
      "11406/11414 [============================>.] - ETA: 0s - loss: 156780.5938 - mse: 156780.5938 - mae: 226.7540\n",
      "Epoch 75: val_loss did not improve from 156317.92188\n",
      "11414/11414 [==============================] - 32s 3ms/step - loss: 156871.5625 - mse: 156871.5625 - mae: 226.7743 - val_loss: 156318.6562 - val_mse: 156318.6562 - val_mae: 226.3350\n",
      "Epoch 76/100\n",
      "11400/11414 [============================>.] - ETA: 0s - loss: 156924.9219 - mse: 156924.9219 - mae: 226.7230\n",
      "Epoch 76: val_loss did not improve from 156317.92188\n",
      "11414/11414 [==============================] - 38s 3ms/step - loss: 156871.8594 - mse: 156871.8594 - mae: 226.7202 - val_loss: 156319.1094 - val_mse: 156319.1094 - val_mae: 226.2346\n",
      "Epoch 77/100\n",
      "11412/11414 [============================>.] - ETA: 0s - loss: 156888.7500 - mse: 156888.7500 - mae: 226.6373\n",
      "Epoch 77: val_loss did not improve from 156317.92188\n",
      "11414/11414 [==============================] - 41s 4ms/step - loss: 156873.0000 - mse: 156873.0000 - mae: 226.6289 - val_loss: 156320.9219 - val_mse: 156320.9219 - val_mae: 226.0031\n",
      "Epoch 78/100\n",
      "11413/11414 [============================>.] - ETA: 0s - loss: 156875.7344 - mse: 156875.7344 - mae: 226.6768\n",
      "Epoch 78: val_loss did not improve from 156317.92188\n",
      "11414/11414 [==============================] - 36s 3ms/step - loss: 156871.5469 - mse: 156871.5469 - mae: 226.6754 - val_loss: 156325.2656 - val_mse: 156325.2656 - val_mae: 225.6124\n",
      "Epoch 79/100\n",
      "11396/11414 [============================>.] - ETA: 0s - loss: 156887.2344 - mse: 156887.2344 - mae: 226.5944\n",
      "Epoch 79: val_loss did not improve from 156317.92188\n",
      "11414/11414 [==============================] - 32s 3ms/step - loss: 156873.0781 - mse: 156873.0781 - mae: 226.5954 - val_loss: 156319.7500 - val_mse: 156319.7500 - val_mae: 226.1392\n",
      "Epoch 80/100\n",
      "11405/11414 [============================>.] - ETA: 0s - loss: 156898.5469 - mse: 156898.5469 - mae: 226.6686\n",
      "Epoch 80: val_loss did not improve from 156317.92188\n",
      "11414/11414 [==============================] - 31s 3ms/step - loss: 156872.0000 - mse: 156872.0000 - mae: 226.6581 - val_loss: 156319.2969 - val_mse: 156319.2969 - val_mae: 226.1918\n",
      "Epoch 81/100\n",
      "11412/11414 [============================>.] - ETA: 0s - loss: 156868.5000 - mse: 156868.5000 - mae: 226.8153\n",
      "Epoch 81: val_loss did not improve from 156317.92188\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 156869.7500 - mse: 156869.7500 - mae: 226.8236 - val_loss: 156325.6250 - val_mse: 156325.6250 - val_mae: 225.5793\n",
      "Epoch 82/100\n",
      "11401/11414 [============================>.] - ETA: 0s - loss: 156847.1094 - mse: 156847.1094 - mae: 226.4938\n",
      "Epoch 82: val_loss did not improve from 156317.92188\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 156874.1719 - mse: 156874.1719 - mae: 226.4908 - val_loss: 156318.2812 - val_mse: 156318.2812 - val_mae: 226.4552\n",
      "Epoch 83/100\n",
      "11386/11414 [============================>.] - ETA: 0s - loss: 155286.1094 - mse: 155286.1094 - mae: 226.7121\n",
      "Epoch 83: val_loss did not improve from 156317.92188\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 156871.1406 - mse: 156871.1406 - mae: 226.7879 - val_loss: 156325.5000 - val_mse: 156325.5000 - val_mae: 225.5998\n",
      "Epoch 84/100\n",
      "11409/11414 [============================>.] - ETA: 0s - loss: 156897.5625 - mse: 156897.5625 - mae: 226.7716\n",
      "Epoch 84: val_loss did not improve from 156317.92188\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 156870.2344 - mse: 156870.2344 - mae: 226.7653 - val_loss: 156326.5625 - val_mse: 156326.5625 - val_mae: 225.5191\n",
      "Epoch 85/100\n",
      "11396/11414 [============================>.] - ETA: 0s - loss: 156805.7812 - mse: 156805.7812 - mae: 226.5656\n",
      "Epoch 85: val_loss did not improve from 156317.92188\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 156872.4375 - mse: 156872.4375 - mae: 226.5720 - val_loss: 156323.6719 - val_mse: 156323.6719 - val_mae: 225.7259\n",
      "Epoch 86/100\n",
      "11409/11414 [============================>.] - ETA: 0s - loss: 156907.5625 - mse: 156907.5625 - mae: 226.5319\n",
      "Epoch 86: val_loss did not improve from 156317.92188\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 156873.7969 - mse: 156873.7969 - mae: 226.5213 - val_loss: 156317.9531 - val_mse: 156317.9531 - val_mae: 226.8198\n",
      "Epoch 87/100\n",
      "11407/11414 [============================>.] - ETA: 0s - loss: 156915.2344 - mse: 156915.2344 - mae: 226.7942\n",
      "Epoch 87: val_loss did not improve from 156317.92188\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 156871.2969 - mse: 156871.2969 - mae: 226.7835 - val_loss: 156319.4375 - val_mse: 156319.4375 - val_mae: 226.1671\n",
      "Epoch 88/100\n",
      "11394/11414 [============================>.] - ETA: 0s - loss: 156996.5938 - mse: 156996.5938 - mae: 226.6952\n",
      "Epoch 88: val_loss did not improve from 156317.92188\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 156871.7656 - mse: 156871.7656 - mae: 226.6639 - val_loss: 156318.4688 - val_mse: 156318.4688 - val_mae: 226.3481\n",
      "Epoch 89/100\n",
      "11398/11414 [============================>.] - ETA: 0s - loss: 156896.6875 - mse: 156896.6875 - mae: 226.6242\n",
      "Epoch 89: val_loss did not improve from 156317.92188\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 156873.3906 - mse: 156873.3906 - mae: 226.6249 - val_loss: 156318.8750 - val_mse: 156318.8750 - val_mae: 226.2530\n",
      "Epoch 90/100\n",
      "11407/11414 [============================>.] - ETA: 0s - loss: 156804.8125 - mse: 156804.8125 - mae: 226.6796\n",
      "Epoch 90: val_loss did not improve from 156317.92188\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 156872.2656 - mse: 156872.2656 - mae: 226.6891 - val_loss: 156320.0156 - val_mse: 156320.0156 - val_mae: 226.0508\n",
      "Epoch 91/100\n",
      "11398/11414 [============================>.] - ETA: 0s - loss: 156882.9844 - mse: 156882.9844 - mae: 226.6774\n",
      "Epoch 91: val_loss did not improve from 156317.92188\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 156872.2656 - mse: 156872.2656 - mae: 226.6730 - val_loss: 156319.8906 - val_mse: 156319.8906 - val_mae: 226.0915\n",
      "Epoch 92/100\n",
      "11395/11414 [============================>.] - ETA: 0s - loss: 156825.1562 - mse: 156825.1562 - mae: 226.5496\n",
      "Epoch 92: val_loss did not improve from 156317.92188\n",
      "11414/11414 [==============================] - 32s 3ms/step - loss: 156872.6250 - mse: 156872.6250 - mae: 226.5592 - val_loss: 156318.1875 - val_mse: 156318.1875 - val_mae: 226.4822\n",
      "Epoch 93/100\n",
      "11404/11414 [============================>.] - ETA: 0s - loss: 156883.6406 - mse: 156883.6406 - mae: 226.7949\n",
      "Epoch 93: val_loss did not improve from 156317.92188\n",
      "11414/11414 [==============================] - 33s 3ms/step - loss: 156868.5000 - mse: 156868.5000 - mae: 226.7965 - val_loss: 156332.1875 - val_mse: 156332.1875 - val_mae: 225.2048\n",
      "Epoch 94/100\n",
      "11396/11414 [============================>.] - ETA: 0s - loss: 156906.1719 - mse: 156906.1719 - mae: 226.5591\n",
      "Epoch 94: val_loss did not improve from 156317.92188\n",
      "11414/11414 [==============================] - 32s 3ms/step - loss: 156874.6719 - mse: 156874.6719 - mae: 226.5499 - val_loss: 156328.3125 - val_mse: 156328.3125 - val_mae: 225.4061\n",
      "Epoch 95/100\n",
      "11409/11414 [============================>.] - ETA: 0s - loss: 156906.6562 - mse: 156906.6562 - mae: 226.5933\n",
      "Epoch 95: val_loss did not improve from 156317.92188\n",
      "11414/11414 [==============================] - 32s 3ms/step - loss: 156873.6250 - mse: 156873.6250 - mae: 226.5805 - val_loss: 156321.0000 - val_mse: 156321.0000 - val_mae: 225.9797\n",
      "Epoch 96/100\n",
      "11408/11414 [============================>.] - ETA: 0s - loss: 156900.5156 - mse: 156900.5156 - mae: 226.7144\n",
      "Epoch 96: val_loss did not improve from 156317.92188\n",
      "11414/11414 [==============================] - 32s 3ms/step - loss: 156870.9219 - mse: 156870.9219 - mae: 226.7060 - val_loss: 156319.3125 - val_mse: 156319.3125 - val_mae: 226.1911\n",
      "Epoch 97/100\n",
      "11402/11414 [============================>.] - ETA: 0s - loss: 156835.6719 - mse: 156835.6719 - mae: 226.5387\n",
      "Epoch 97: val_loss improved from 156317.92188 to 156317.84375, saving model to my_checkpoint.ckpt\n",
      "11414/11414 [==============================] - 32s 3ms/step - loss: 156873.5312 - mse: 156873.5312 - mae: 226.5499 - val_loss: 156317.8438 - val_mse: 156317.8438 - val_mae: 226.6845\n",
      "Epoch 98/100\n",
      "11410/11414 [============================>.] - ETA: 0s - loss: 156893.2969 - mse: 156893.2969 - mae: 226.7872\n",
      "Epoch 98: val_loss did not improve from 156317.84375\n",
      "11414/11414 [==============================] - 33s 3ms/step - loss: 156871.3906 - mse: 156871.3906 - mae: 226.7832 - val_loss: 156321.4219 - val_mse: 156321.4219 - val_mae: 225.9351\n",
      "Epoch 99/100\n",
      "11396/11414 [============================>.] - ETA: 0s - loss: 156819.5469 - mse: 156819.5469 - mae: 226.7174\n",
      "Epoch 99: val_loss did not improve from 156317.84375\n",
      "11414/11414 [==============================] - 31s 3ms/step - loss: 156870.5156 - mse: 156870.5156 - mae: 226.7122 - val_loss: 156328.6875 - val_mse: 156328.6875 - val_mae: 225.3882\n",
      "Epoch 100/100\n",
      "11397/11414 [============================>.] - ETA: 0s - loss: 156937.4688 - mse: 156937.4688 - mae: 226.5814\n",
      "Epoch 100: val_loss did not improve from 156317.84375\n",
      "11414/11414 [==============================] - 33s 3ms/step - loss: 156873.3125 - mse: 156873.3125 - mae: 226.5706 - val_loss: 156318.4062 - val_mse: 156318.4062 - val_mae: 226.3595\n"
     ]
    }
   ],
   "source": [
    "# 최적 시점 저장하기 위해  my_checkpoint.ckpt 를 통해 가장 최적 포인트가 저장함.\n",
    "checkpoint_path = \"my_checkpoint.ckpt\"\n",
    "\n",
    "# 모델의 가중치를 저장하는 콜백 만들기\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath = checkpoint_path, save_weights_only=True, save_best_only=True, mointor='val_loss',verbose = 1)\n",
    "\n",
    "\n",
    "# 새로운 콜백으로 모델 훈련하기\n",
    "hist = model_dl.fit(train_x, train_y, epochs = 100, validation_data = (test_x, test_y), callbacks = [cp_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1629e559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2854/2854 - 4s - loss: 156317.8438 - mse: 156317.8438 - mae: 226.6845 - 4s/epoch - 1ms/step\n"
     ]
    }
   ],
   "source": [
    "# 가중치 로드\n",
    "model_dl.load_weights(checkpoint_path)\n",
    "\n",
    "# 모델 재평가\n",
    "loss = model_dl.evaluate(test_x, test_y, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d5f0c75c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2854/2854 [==============================] - 5s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "# y = .predict\n",
    "val_pred = model_dl.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7d2ca8fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "395.3706407421557"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mse 구하기\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(test_y, val_pred, squared = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd962228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE : 19.88392920783404\n"
     ]
    }
   ],
   "source": [
    "print(\"RMSE :\", 395.3706407421557**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0d11bdfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-7.679010494143768e-08"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# r2 구하기\n",
    "from sklearn.metrics import r2_score\n",
    "r2_score(test_y, val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e0d4a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "soyeon",
   "language": "python",
   "name": "soyeon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
