{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bc97c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88a99c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:\\\\Users\\\\syeon\\\\!soyeon\\\\archive\\\\train_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f455f0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>week</th>\n",
       "      <th>center_id</th>\n",
       "      <th>city_code</th>\n",
       "      <th>region_code</th>\n",
       "      <th>op_area</th>\n",
       "      <th>meal_id</th>\n",
       "      <th>checkout_price</th>\n",
       "      <th>base_price</th>\n",
       "      <th>emailer_for_promotion</th>\n",
       "      <th>...</th>\n",
       "      <th>category_Rice Bowl</th>\n",
       "      <th>category_Salad</th>\n",
       "      <th>category_Sandwich</th>\n",
       "      <th>category_Seafood</th>\n",
       "      <th>category_Soup</th>\n",
       "      <th>category_Starters</th>\n",
       "      <th>cuisine_Continental</th>\n",
       "      <th>cuisine_Indian</th>\n",
       "      <th>cuisine_Italian</th>\n",
       "      <th>cuisine_Thai</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1379560</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>647</td>\n",
       "      <td>56</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1885</td>\n",
       "      <td>136.83</td>\n",
       "      <td>152.29</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1466964</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>647</td>\n",
       "      <td>56</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1993</td>\n",
       "      <td>136.83</td>\n",
       "      <td>135.83</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1346989</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>647</td>\n",
       "      <td>56</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2539</td>\n",
       "      <td>134.86</td>\n",
       "      <td>135.86</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1338232</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>647</td>\n",
       "      <td>56</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2139</td>\n",
       "      <td>339.50</td>\n",
       "      <td>437.53</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1448490</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>647</td>\n",
       "      <td>56</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2631</td>\n",
       "      <td>243.50</td>\n",
       "      <td>242.50</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456543</th>\n",
       "      <td>1271326</td>\n",
       "      <td>145</td>\n",
       "      <td>61</td>\n",
       "      <td>473</td>\n",
       "      <td>77</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1543</td>\n",
       "      <td>484.09</td>\n",
       "      <td>484.09</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456544</th>\n",
       "      <td>1062036</td>\n",
       "      <td>145</td>\n",
       "      <td>61</td>\n",
       "      <td>473</td>\n",
       "      <td>77</td>\n",
       "      <td>4.5</td>\n",
       "      <td>2304</td>\n",
       "      <td>482.09</td>\n",
       "      <td>482.09</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456545</th>\n",
       "      <td>1110849</td>\n",
       "      <td>145</td>\n",
       "      <td>61</td>\n",
       "      <td>473</td>\n",
       "      <td>77</td>\n",
       "      <td>4.5</td>\n",
       "      <td>2664</td>\n",
       "      <td>237.68</td>\n",
       "      <td>321.07</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456546</th>\n",
       "      <td>1147725</td>\n",
       "      <td>145</td>\n",
       "      <td>61</td>\n",
       "      <td>473</td>\n",
       "      <td>77</td>\n",
       "      <td>4.5</td>\n",
       "      <td>2569</td>\n",
       "      <td>243.50</td>\n",
       "      <td>313.34</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456547</th>\n",
       "      <td>1361984</td>\n",
       "      <td>145</td>\n",
       "      <td>61</td>\n",
       "      <td>473</td>\n",
       "      <td>77</td>\n",
       "      <td>4.5</td>\n",
       "      <td>2490</td>\n",
       "      <td>292.03</td>\n",
       "      <td>290.03</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>456548 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  week  center_id  city_code  region_code  op_area  meal_id  \\\n",
       "0       1379560     1         55        647           56      2.0     1885   \n",
       "1       1466964     1         55        647           56      2.0     1993   \n",
       "2       1346989     1         55        647           56      2.0     2539   \n",
       "3       1338232     1         55        647           56      2.0     2139   \n",
       "4       1448490     1         55        647           56      2.0     2631   \n",
       "...         ...   ...        ...        ...          ...      ...      ...   \n",
       "456543  1271326   145         61        473           77      4.5     1543   \n",
       "456544  1062036   145         61        473           77      4.5     2304   \n",
       "456545  1110849   145         61        473           77      4.5     2664   \n",
       "456546  1147725   145         61        473           77      4.5     2569   \n",
       "456547  1361984   145         61        473           77      4.5     2490   \n",
       "\n",
       "        checkout_price  base_price  emailer_for_promotion  ...  \\\n",
       "0               136.83      152.29                      0  ...   \n",
       "1               136.83      135.83                      0  ...   \n",
       "2               134.86      135.86                      0  ...   \n",
       "3               339.50      437.53                      0  ...   \n",
       "4               243.50      242.50                      0  ...   \n",
       "...                ...         ...                    ...  ...   \n",
       "456543          484.09      484.09                      0  ...   \n",
       "456544          482.09      482.09                      0  ...   \n",
       "456545          237.68      321.07                      0  ...   \n",
       "456546          243.50      313.34                      0  ...   \n",
       "456547          292.03      290.03                      0  ...   \n",
       "\n",
       "        category_Rice Bowl  category_Salad  category_Sandwich  \\\n",
       "0                      0.0             0.0                0.0   \n",
       "1                      0.0             0.0                0.0   \n",
       "2                      0.0             0.0                0.0   \n",
       "3                      0.0             0.0                0.0   \n",
       "4                      0.0             0.0                0.0   \n",
       "...                    ...             ...                ...   \n",
       "456543                 0.0             0.0                0.0   \n",
       "456544                 0.0             0.0                0.0   \n",
       "456545                 0.0             1.0                0.0   \n",
       "456546                 0.0             1.0                0.0   \n",
       "456547                 0.0             1.0                0.0   \n",
       "\n",
       "        category_Seafood  category_Soup  category_Starters  \\\n",
       "0                    0.0            0.0                0.0   \n",
       "1                    0.0            0.0                0.0   \n",
       "2                    0.0            0.0                0.0   \n",
       "3                    0.0            0.0                0.0   \n",
       "4                    0.0            0.0                0.0   \n",
       "...                  ...            ...                ...   \n",
       "456543               0.0            0.0                0.0   \n",
       "456544               0.0            0.0                0.0   \n",
       "456545               0.0            0.0                0.0   \n",
       "456546               0.0            0.0                0.0   \n",
       "456547               0.0            0.0                0.0   \n",
       "\n",
       "        cuisine_Continental  cuisine_Indian  cuisine_Italian  cuisine_Thai  \n",
       "0                       0.0             0.0              0.0           1.0  \n",
       "1                       0.0             0.0              0.0           1.0  \n",
       "2                       0.0             0.0              0.0           1.0  \n",
       "3                       0.0             1.0              0.0           0.0  \n",
       "4                       0.0             1.0              0.0           0.0  \n",
       "...                     ...             ...              ...           ...  \n",
       "456543                  0.0             1.0              0.0           0.0  \n",
       "456544                  0.0             1.0              0.0           0.0  \n",
       "456545                  0.0             0.0              1.0           0.0  \n",
       "456546                  0.0             0.0              1.0           0.0  \n",
       "456547                  0.0             0.0              1.0           0.0  \n",
       "\n",
       "[456548 rows x 33 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "# fit_transform은 train에만 사용하고 test에는 학습된 인코더에 fit만 해야한다\n",
    "data_cat = ohe.fit_transform(df[['center_type']])\n",
    "df_center = pd.concat([df.drop(columns=['center_type']),pd.DataFrame(data_cat, columns=['center_type_' + str(col) for col in ohe.categories_[0]])], axis=1)\n",
    "\n",
    "data_cat = ohe.fit_transform(df[['category']])\n",
    "df_category = pd.concat([df_center.drop(columns=['category']),pd.DataFrame(data_cat, columns=['category_' + str(col) for col in ohe.categories_[0]])], axis=1)\n",
    "\n",
    "data_cat = ohe.fit_transform(df[['cuisine']])\n",
    "train_df = pd.concat([df_category.drop(columns=['cuisine']),pd.DataFrame(data_cat, columns=['cuisine_' + str(col) for col in ohe.categories_[0]])], axis=1)\n",
    "\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d6465d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "388a5456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features와 target 분리\n",
    "X = train_df.drop('num_orders', axis=1)\n",
    "y = train_df['num_orders']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47e53549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, test를 8:2로 나누기\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8dde80d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 5개를 임포트하세요 (Dense, tf, seauential, Modelcheckpoint)\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c813c869",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(365238, 32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x의 입력 갯수 확인\n",
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c26f9ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델 정의 및 입력 (4개의 입력 받아, n개의 퍼셉트론 구성된 레이어 제작)\n",
    "model_dl = tf.keras.Sequential()\n",
    "model_dl.add(tf.keras.layers.Dense(128, activation = 'relu', input_shape = [32]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dcee7b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#덴스레이어 쌓기 (relu 또는 swish)\n",
    "model_dl.add(tf.keras.layers.Dense(64, activation = 'relu'))\n",
    "model_dl.add(tf.keras.layers.Dense(32, activation = 'relu'))\n",
    "model_dl.add(tf.keras.layers.Dense(16, activation = 'relu'))\n",
    "model_dl.add(tf.keras.layers.Dense(8, activation = 'relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8899925e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#마지막 출력 레이어 확인 (리그레션이라 없음)\n",
    "model_dl.add(tf.keras.layers.Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8646178b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 컴파일 지정하기\n",
    "optimizer = tf.keras.optimizers.RMSprop()\n",
    "model_dl.compile(optimizer,\n",
    "                 loss = 'mse',\n",
    "                 metrics = ['mse', 'mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "72a0d31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_6 (Dense)             (None, 128)               4224      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 16)                528       \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15,233\n",
      "Trainable params: 15,233\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_dl.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e22aab32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11414/11414 [==============================] - 32s 3ms/step - loss: 350304.6250 - mse: 350304.6250 - mae: 291.5761 - val_loss: 219815.0625 - val_mse: 219815.0625 - val_mae: 251.9837\n",
      "Epoch 2/100\n",
      "11414/11414 [==============================] - 31s 3ms/step - loss: 218171.0156 - mse: 218171.0156 - mae: 247.8486 - val_loss: 214401.3750 - val_mse: 214401.3750 - val_mae: 241.9808\n",
      "Epoch 3/100\n",
      "11414/11414 [==============================] - 31s 3ms/step - loss: 212855.3125 - mse: 212855.3125 - mae: 238.5798 - val_loss: 209248.1562 - val_mse: 209248.1562 - val_mae: 233.4241\n",
      "Epoch 4/100\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 207812.2031 - mse: 207812.2031 - mae: 230.7502 - val_loss: 204355.3594 - val_mse: 204355.3594 - val_mae: 226.0824\n",
      "Epoch 5/100\n",
      "11414/11414 [==============================] - 30s 3ms/step - loss: 203029.4844 - mse: 203029.4844 - mae: 224.1004 - val_loss: 199724.6562 - val_mse: 199724.6562 - val_mae: 220.1055\n",
      "Epoch 6/100\n",
      "11414/11414 [==============================] - 30s 3ms/step - loss: 198498.2188 - mse: 198498.2188 - mae: 218.5971 - val_loss: 195344.1250 - val_mse: 195344.1250 - val_mae: 215.2513\n",
      "Epoch 7/100\n",
      "11414/11414 [==============================] - 31s 3ms/step - loss: 194232.1406 - mse: 194232.1406 - mae: 214.0478 - val_loss: 191226.4375 - val_mse: 191226.4375 - val_mae: 211.1938\n",
      "Epoch 8/100\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 190220.4375 - mse: 190220.4375 - mae: 210.3886 - val_loss: 187368.8125 - val_mse: 187368.8125 - val_mae: 207.9521\n",
      "Epoch 9/100\n",
      "11414/11414 [==============================] - 30s 3ms/step - loss: 186461.6406 - mse: 186461.6406 - mae: 207.5404 - val_loss: 183763.9219 - val_mse: 183763.9219 - val_mae: 205.3658\n",
      "Epoch 10/100\n",
      "11414/11414 [==============================] - 30s 3ms/step - loss: 182961.4375 - mse: 182961.4375 - mae: 205.3170 - val_loss: 180406.8125 - val_mse: 180406.8125 - val_mae: 203.5459\n",
      "Epoch 11/100\n",
      "11414/11414 [==============================] - 28s 2ms/step - loss: 179712.6406 - mse: 179712.6406 - mae: 203.7467 - val_loss: 177304.1562 - val_mse: 177304.1562 - val_mae: 202.3162\n",
      "Epoch 12/100\n",
      "11414/11414 [==============================] - 31s 3ms/step - loss: 176717.6875 - mse: 176717.6875 - mae: 202.7101 - val_loss: 174450.6875 - val_mse: 174450.6875 - val_mae: 201.5146\n",
      "Epoch 13/100\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 173954.1562 - mse: 173954.1562 - mae: 202.1664 - val_loss: 171852.2031 - val_mse: 171852.2031 - val_mae: 201.1555\n",
      "Epoch 14/100\n",
      "11414/11414 [==============================] - 32s 3ms/step - loss: 171455.1094 - mse: 171455.1094 - mae: 202.0905 - val_loss: 169496.8281 - val_mse: 169496.8281 - val_mae: 201.2830\n",
      "Epoch 15/100\n",
      "11414/11414 [==============================] - 28s 2ms/step - loss: 169191.7344 - mse: 169191.7344 - mae: 202.3327 - val_loss: 167373.0781 - val_mse: 167373.0781 - val_mae: 201.7724\n",
      "Epoch 16/100\n",
      "11414/11414 [==============================] - 27s 2ms/step - loss: 167177.4375 - mse: 167177.4375 - mae: 202.9856 - val_loss: 165489.2969 - val_mse: 165489.2969 - val_mae: 202.5522\n",
      "Epoch 17/100\n",
      "11414/11414 [==============================] - 27s 2ms/step - loss: 165389.6562 - mse: 165389.6562 - mae: 203.9533 - val_loss: 163833.6875 - val_mse: 163833.6875 - val_mae: 203.6067\n",
      "Epoch 18/100\n",
      "11414/11414 [==============================] - 27s 2ms/step - loss: 163826.6719 - mse: 163826.6719 - mae: 205.1094 - val_loss: 162399.2500 - val_mse: 162399.2500 - val_mae: 204.9535\n",
      "Epoch 19/100\n",
      "11414/11414 [==============================] - 30s 3ms/step - loss: 162475.2188 - mse: 162475.2188 - mae: 206.4890 - val_loss: 161170.3906 - val_mse: 161170.3906 - val_mae: 206.4182\n",
      "Epoch 20/100\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 161326.0156 - mse: 161326.0156 - mae: 208.0419 - val_loss: 160140.8125 - val_mse: 160140.8125 - val_mae: 208.0379\n",
      "Epoch 21/100\n",
      "11414/11414 [==============================] - 33s 3ms/step - loss: 160385.0625 - mse: 160385.0625 - mae: 209.6062 - val_loss: 159301.6250 - val_mse: 159301.6250 - val_mae: 209.6680\n",
      "Epoch 22/100\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 159600.7969 - mse: 159600.7969 - mae: 211.2517 - val_loss: 158616.6875 - val_mse: 158616.6875 - val_mae: 211.3188\n",
      "Epoch 23/100\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 158976.2969 - mse: 158976.2969 - mae: 212.8419 - val_loss: 158071.6875 - val_mse: 158071.6875 - val_mae: 212.9402\n",
      "Epoch 24/100\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 158482.1250 - mse: 158482.1250 - mae: 214.4467 - val_loss: 157646.1250 - val_mse: 157646.1250 - val_mae: 214.4839\n",
      "Epoch 25/100\n",
      "11414/11414 [==============================] - 30s 3ms/step - loss: 158093.4844 - mse: 158093.4844 - mae: 215.8809 - val_loss: 157314.5312 - val_mse: 157314.5312 - val_mae: 215.8766\n",
      "Epoch 26/100\n",
      "11414/11414 [==============================] - 30s 3ms/step - loss: 157793.4375 - mse: 157793.4375 - mae: 217.2182 - val_loss: 157063.1250 - val_mse: 157063.1250 - val_mae: 217.2170\n",
      "Epoch 27/100\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 157565.8281 - mse: 157565.8281 - mae: 218.4949 - val_loss: 156874.0938 - val_mse: 156874.0938 - val_mae: 218.4049\n",
      "Epoch 28/100\n",
      "11414/11414 [==============================] - 31s 3ms/step - loss: 157395.0156 - mse: 157395.0156 - mae: 219.5929 - val_loss: 156733.9531 - val_mse: 156733.9531 - val_mae: 219.4193\n",
      "Epoch 29/100\n",
      "11414/11414 [==============================] - 30s 3ms/step - loss: 157267.0625 - mse: 157267.0625 - mae: 220.5086 - val_loss: 156631.1719 - val_mse: 156631.1719 - val_mae: 220.3029\n",
      "Epoch 30/100\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 157171.6719 - mse: 157171.6719 - mae: 221.3685 - val_loss: 156551.3125 - val_mse: 156551.3125 - val_mae: 221.1414\n",
      "Epoch 31/100\n",
      "11414/11414 [==============================] - 27s 2ms/step - loss: 157099.2500 - mse: 157099.2500 - mae: 222.1266 - val_loss: 156494.2969 - val_mse: 156494.2969 - val_mae: 221.8478\n",
      "Epoch 32/100\n",
      "11414/11414 [==============================] - 28s 2ms/step - loss: 157044.7500 - mse: 157044.7500 - mae: 222.7981 - val_loss: 156450.2969 - val_mse: 156450.2969 - val_mae: 222.4732\n",
      "Epoch 33/100\n",
      "11414/11414 [==============================] - 30s 3ms/step - loss: 157003.7969 - mse: 157003.7969 - mae: 223.3650 - val_loss: 156418.5000 - val_mse: 156418.5000 - val_mae: 222.9869\n",
      "Epoch 34/100\n",
      "11414/11414 [==============================] - 30s 3ms/step - loss: 156973.5625 - mse: 156973.5625 - mae: 223.8248 - val_loss: 156394.7500 - val_mse: 156394.7500 - val_mae: 223.4308\n",
      "Epoch 35/100\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 156949.5312 - mse: 156949.5312 - mae: 224.2565 - val_loss: 156377.3750 - val_mse: 156377.3750 - val_mae: 223.8090\n",
      "Epoch 36/100\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 156932.6406 - mse: 156932.6406 - mae: 224.5810 - val_loss: 156363.9062 - val_mse: 156363.9062 - val_mae: 224.1316\n",
      "Epoch 37/100\n",
      "11414/11414 [==============================] - 28s 2ms/step - loss: 156919.2812 - mse: 156919.2812 - mae: 224.8779 - val_loss: 156353.8438 - val_mse: 156353.8438 - val_mae: 224.4075\n",
      "Epoch 38/100\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 156909.1094 - mse: 156909.1094 - mae: 225.1475 - val_loss: 156346.5312 - val_mse: 156346.5312 - val_mae: 224.6402\n",
      "Epoch 39/100\n",
      "11414/11414 [==============================] - 33s 3ms/step - loss: 156901.3281 - mse: 156901.3281 - mae: 225.3407 - val_loss: 156340.4219 - val_mse: 156340.4219 - val_mae: 224.8424\n",
      "Epoch 40/100\n",
      "11414/11414 [==============================] - 32s 3ms/step - loss: 156894.7656 - mse: 156894.7656 - mae: 225.5403 - val_loss: 156335.9844 - val_mse: 156335.9844 - val_mae: 225.0199\n",
      "Epoch 41/100\n",
      "11414/11414 [==============================] - 34s 3ms/step - loss: 156890.4844 - mse: 156890.4844 - mae: 225.6970 - val_loss: 156332.8750 - val_mse: 156332.8750 - val_mae: 225.1677\n",
      "Epoch 42/100\n",
      "11414/11414 [==============================] - 28s 2ms/step - loss: 156886.6094 - mse: 156886.6094 - mae: 225.8246 - val_loss: 156330.8125 - val_mse: 156330.8125 - val_mae: 225.2758\n",
      "Epoch 43/100\n",
      "11414/11414 [==============================] - 28s 2ms/step - loss: 156884.2344 - mse: 156884.2344 - mae: 225.9416 - val_loss: 156328.6562 - val_mse: 156328.6562 - val_mae: 225.3853\n",
      "Epoch 44/100\n",
      "11414/11414 [==============================] - 28s 2ms/step - loss: 156881.5469 - mse: 156881.5469 - mae: 226.0506 - val_loss: 156327.0312 - val_mse: 156327.0312 - val_mae: 225.4842\n",
      "Epoch 45/100\n",
      "11414/11414 [==============================] - 28s 2ms/step - loss: 156879.4062 - mse: 156879.4062 - mae: 226.1244 - val_loss: 156325.8125 - val_mse: 156325.8125 - val_mae: 225.5688\n",
      "Epoch 46/100\n",
      "11414/11414 [==============================] - 28s 2ms/step - loss: 156877.7188 - mse: 156877.7188 - mae: 226.2287 - val_loss: 156324.8438 - val_mse: 156324.8438 - val_mae: 225.6485\n",
      "Epoch 47/100\n",
      "11414/11414 [==============================] - 27s 2ms/step - loss: 156876.5781 - mse: 156876.5781 - mae: 226.2788 - val_loss: 156323.8906 - val_mse: 156323.8906 - val_mae: 225.7093\n",
      "Epoch 48/100\n",
      "11414/11414 [==============================] - 31s 3ms/step - loss: 156875.4219 - mse: 156875.4219 - mae: 226.3339 - val_loss: 156323.0781 - val_mse: 156323.0781 - val_mae: 225.7674\n",
      "Epoch 49/100\n",
      "11414/11414 [==============================] - 28s 2ms/step - loss: 156874.9062 - mse: 156874.9062 - mae: 226.3738 - val_loss: 156322.7188 - val_mse: 156322.7188 - val_mae: 225.8114\n",
      "Epoch 50/100\n",
      "11414/11414 [==============================] - 28s 2ms/step - loss: 156874.0781 - mse: 156874.0781 - mae: 226.4253 - val_loss: 156322.0000 - val_mse: 156322.0000 - val_mae: 225.8477\n",
      "Epoch 51/100\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 156873.2188 - mse: 156873.2188 - mae: 226.4494 - val_loss: 156321.6406 - val_mse: 156321.6406 - val_mae: 225.8838\n",
      "Epoch 52/100\n",
      "11414/11414 [==============================] - 28s 2ms/step - loss: 156872.6406 - mse: 156872.6406 - mae: 226.4841 - val_loss: 156321.6875 - val_mse: 156321.6875 - val_mae: 225.8963\n",
      "Epoch 53/100\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 156872.5469 - mse: 156872.5469 - mae: 226.5242 - val_loss: 156321.3750 - val_mse: 156321.3750 - val_mae: 225.9251\n",
      "Epoch 54/100\n",
      "11414/11414 [==============================] - 28s 2ms/step - loss: 156871.9219 - mse: 156871.9219 - mae: 226.5261 - val_loss: 156321.5000 - val_mse: 156321.5000 - val_mae: 225.9321\n",
      "Epoch 55/100\n",
      "11414/11414 [==============================] - 30s 3ms/step - loss: 156871.5312 - mse: 156871.5312 - mae: 226.5622 - val_loss: 156321.3906 - val_mse: 156321.3906 - val_mae: 225.9513\n",
      "Epoch 56/100\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 156871.5781 - mse: 156871.5781 - mae: 226.5551 - val_loss: 156320.9688 - val_mse: 156320.9688 - val_mae: 225.9741\n",
      "Epoch 57/100\n",
      "11414/11414 [==============================] - 32s 3ms/step - loss: 156871.9219 - mse: 156871.9219 - mae: 226.5573 - val_loss: 156320.9688 - val_mse: 156320.9688 - val_mae: 225.9764\n",
      "Epoch 58/100\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 156871.3438 - mse: 156871.3438 - mae: 226.5923 - val_loss: 156321.0000 - val_mse: 156321.0000 - val_mae: 225.9828\n",
      "Epoch 59/100\n",
      "11414/11414 [==============================] - 32s 3ms/step - loss: 156871.6406 - mse: 156871.6406 - mae: 226.5722 - val_loss: 156320.7656 - val_mse: 156320.7656 - val_mae: 225.9998\n",
      "Epoch 60/100\n",
      "11414/11414 [==============================] - 34s 3ms/step - loss: 156870.9688 - mse: 156870.9688 - mae: 226.5950 - val_loss: 156320.7812 - val_mse: 156320.7812 - val_mae: 225.9964\n",
      "Epoch 61/100\n",
      "11414/11414 [==============================] - 32s 3ms/step - loss: 156870.8438 - mse: 156870.8438 - mae: 226.6010 - val_loss: 156320.8906 - val_mse: 156320.8906 - val_mae: 226.0008\n",
      "Epoch 62/100\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 156871.2188 - mse: 156871.2188 - mae: 226.5815 - val_loss: 156320.7344 - val_mse: 156320.7344 - val_mae: 226.0135\n",
      "Epoch 63/100\n",
      "11414/11414 [==============================] - 35s 3ms/step - loss: 156870.5469 - mse: 156870.5469 - mae: 226.6211 - val_loss: 156320.8594 - val_mse: 156320.8594 - val_mae: 226.0097\n",
      "Epoch 64/100\n",
      "11414/11414 [==============================] - 30s 3ms/step - loss: 156871.2812 - mse: 156871.2812 - mae: 226.6111 - val_loss: 156320.7656 - val_mse: 156320.7656 - val_mae: 226.0122\n",
      "Epoch 65/100\n",
      "11414/11414 [==============================] - 32s 3ms/step - loss: 156871.2344 - mse: 156871.2344 - mae: 226.6094 - val_loss: 156320.8906 - val_mse: 156320.8906 - val_mae: 226.0031\n",
      "Epoch 66/100\n",
      "11414/11414 [==============================] - 28s 2ms/step - loss: 156871.0312 - mse: 156871.0312 - mae: 226.6078 - val_loss: 156320.8125 - val_mse: 156320.8125 - val_mae: 225.9983\n",
      "Epoch 67/100\n",
      "11414/11414 [==============================] - 28s 2ms/step - loss: 156871.3594 - mse: 156871.3594 - mae: 226.5881 - val_loss: 156320.7812 - val_mse: 156320.7812 - val_mae: 226.0104\n",
      "Epoch 68/100\n",
      "11414/11414 [==============================] - 28s 2ms/step - loss: 156870.7656 - mse: 156870.7656 - mae: 226.6267 - val_loss: 156320.8750 - val_mse: 156320.8750 - val_mae: 226.0097\n",
      "Epoch 69/100\n",
      "11414/11414 [==============================] - 28s 2ms/step - loss: 156870.9375 - mse: 156870.9375 - mae: 226.5967 - val_loss: 156320.7500 - val_mse: 156320.7500 - val_mae: 226.0136\n",
      "Epoch 70/100\n",
      "11414/11414 [==============================] - 28s 2ms/step - loss: 156870.8125 - mse: 156870.8125 - mae: 226.6082 - val_loss: 156320.4531 - val_mse: 156320.4531 - val_mae: 226.0254\n",
      "Epoch 71/100\n",
      "11414/11414 [==============================] - 28s 2ms/step - loss: 156870.7031 - mse: 156870.7031 - mae: 226.6236 - val_loss: 156320.2656 - val_mse: 156320.2656 - val_mae: 226.0353\n",
      "Epoch 72/100\n",
      "11414/11414 [==============================] - 28s 2ms/step - loss: 156870.6875 - mse: 156870.6875 - mae: 226.6343 - val_loss: 156320.3594 - val_mse: 156320.3594 - val_mae: 226.0309\n",
      "Epoch 73/100\n",
      "11414/11414 [==============================] - 28s 2ms/step - loss: 156870.7969 - mse: 156870.7969 - mae: 226.6225 - val_loss: 156320.1719 - val_mse: 156320.1719 - val_mae: 226.0373\n",
      "Epoch 74/100\n",
      "11414/11414 [==============================] - 28s 2ms/step - loss: 156870.3125 - mse: 156870.3125 - mae: 226.6388 - val_loss: 156320.1875 - val_mse: 156320.1875 - val_mae: 226.0362\n",
      "Epoch 75/100\n",
      "11414/11414 [==============================] - 28s 2ms/step - loss: 156870.6562 - mse: 156870.6562 - mae: 226.6323 - val_loss: 156320.4062 - val_mse: 156320.4062 - val_mae: 226.0280\n",
      "Epoch 76/100\n",
      "11414/11414 [==============================] - 28s 2ms/step - loss: 156870.4531 - mse: 156870.4531 - mae: 226.6219 - val_loss: 156320.1719 - val_mse: 156320.1719 - val_mae: 226.0373\n",
      "Epoch 77/100\n",
      "11414/11414 [==============================] - 28s 2ms/step - loss: 156870.6406 - mse: 156870.6406 - mae: 226.6254 - val_loss: 156320.1719 - val_mse: 156320.1719 - val_mae: 226.0432\n",
      "Epoch 78/100\n",
      "11414/11414 [==============================] - 27s 2ms/step - loss: 156869.8594 - mse: 156869.8594 - mae: 226.6572 - val_loss: 156320.0156 - val_mse: 156320.0156 - val_mae: 226.0618\n",
      "Epoch 79/100\n",
      "11414/11414 [==============================] - 28s 2ms/step - loss: 156870.6406 - mse: 156870.6406 - mae: 226.6418 - val_loss: 156320.0156 - val_mse: 156320.0156 - val_mae: 226.0605\n",
      "Epoch 80/100\n",
      "11414/11414 [==============================] - 27s 2ms/step - loss: 156870.3281 - mse: 156870.3281 - mae: 226.6539 - val_loss: 156319.9688 - val_mse: 156319.9688 - val_mae: 226.0626\n",
      "Epoch 81/100\n",
      "11414/11414 [==============================] - 27s 2ms/step - loss: 156870.0625 - mse: 156870.0625 - mae: 226.6511 - val_loss: 156319.9688 - val_mse: 156319.9688 - val_mae: 226.0683\n",
      "Epoch 82/100\n",
      "11414/11414 [==============================] - 28s 2ms/step - loss: 156869.9219 - mse: 156869.9219 - mae: 226.6596 - val_loss: 156320.0156 - val_mse: 156320.0156 - val_mae: 226.0617\n",
      "Epoch 83/100\n",
      "11414/11414 [==============================] - 27s 2ms/step - loss: 156870.1250 - mse: 156870.1250 - mae: 226.6630 - val_loss: 156319.9688 - val_mse: 156319.9688 - val_mae: 226.0625\n",
      "Epoch 84/100\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 156870.5781 - mse: 156870.5781 - mae: 226.6660 - val_loss: 156319.9375 - val_mse: 156319.9375 - val_mae: 226.0571\n",
      "Epoch 85/100\n",
      "11414/11414 [==============================] - 32s 3ms/step - loss: 156870.7344 - mse: 156870.7344 - mae: 226.6473 - val_loss: 156319.9844 - val_mse: 156319.9844 - val_mae: 226.0536\n",
      "Epoch 86/100\n",
      "11414/11414 [==============================] - 31s 3ms/step - loss: 156870.2188 - mse: 156870.2188 - mae: 226.6625 - val_loss: 156319.9688 - val_mse: 156319.9688 - val_mae: 226.0626\n",
      "Epoch 87/100\n",
      "11414/11414 [==============================] - 30s 3ms/step - loss: 156870.2969 - mse: 156870.2969 - mae: 226.6752 - val_loss: 156320.0156 - val_mse: 156320.0156 - val_mae: 226.0687\n",
      "Epoch 88/100\n",
      "11414/11414 [==============================] - 26s 2ms/step - loss: 156870.2969 - mse: 156870.2969 - mae: 226.6600 - val_loss: 156319.9531 - val_mse: 156319.9531 - val_mae: 226.0625\n",
      "Epoch 89/100\n",
      "11414/11414 [==============================] - 26s 2ms/step - loss: 156870.2188 - mse: 156870.2188 - mae: 226.6628 - val_loss: 156319.9531 - val_mse: 156319.9531 - val_mae: 226.0683\n",
      "Epoch 90/100\n",
      "11414/11414 [==============================] - 26s 2ms/step - loss: 156870.5000 - mse: 156870.5000 - mae: 226.6508 - val_loss: 156320.0156 - val_mse: 156320.0156 - val_mae: 226.0622\n",
      "Epoch 91/100\n",
      "11414/11414 [==============================] - 28s 2ms/step - loss: 156869.6250 - mse: 156869.6250 - mae: 226.6763 - val_loss: 156320.0156 - val_mse: 156320.0156 - val_mae: 226.0605\n",
      "Epoch 92/100\n",
      "11414/11414 [==============================] - 28s 2ms/step - loss: 156870.2969 - mse: 156870.2969 - mae: 226.6516 - val_loss: 156320.0000 - val_mse: 156320.0000 - val_mae: 226.0536\n",
      "Epoch 93/100\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 156869.7344 - mse: 156869.7344 - mae: 226.6695 - val_loss: 156320.0781 - val_mse: 156320.0781 - val_mae: 226.0453\n",
      "Epoch 94/100\n",
      "11414/11414 [==============================] - 28s 2ms/step - loss: 156870.6562 - mse: 156870.6562 - mae: 226.6252 - val_loss: 156320.1406 - val_mse: 156320.1406 - val_mae: 226.0433\n",
      "Epoch 95/100\n",
      "11414/11414 [==============================] - 28s 2ms/step - loss: 156870.3594 - mse: 156870.3594 - mae: 226.6462 - val_loss: 156320.0781 - val_mse: 156320.0781 - val_mae: 226.0473\n",
      "Epoch 96/100\n",
      "11414/11414 [==============================] - 28s 2ms/step - loss: 156870.1562 - mse: 156870.1562 - mae: 226.6629 - val_loss: 156320.2344 - val_mse: 156320.2344 - val_mae: 226.0387\n",
      "Epoch 97/100\n",
      "11414/11414 [==============================] - 28s 2ms/step - loss: 156870.5000 - mse: 156870.5000 - mae: 226.6276 - val_loss: 156320.4219 - val_mse: 156320.4219 - val_mae: 226.0262\n",
      "Epoch 98/100\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 156870.9531 - mse: 156870.9531 - mae: 226.6171 - val_loss: 156320.3594 - val_mse: 156320.3594 - val_mae: 226.0252\n",
      "Epoch 99/100\n",
      "11414/11414 [==============================] - 31s 3ms/step - loss: 156870.5938 - mse: 156870.5938 - mae: 226.6293 - val_loss: 156320.2969 - val_mse: 156320.2969 - val_mae: 226.0319\n",
      "Epoch 100/100\n",
      "11414/11414 [==============================] - 34s 3ms/step - loss: 156871.0938 - mse: 156871.0938 - mae: 226.6143 - val_loss: 156320.4062 - val_mse: 156320.4062 - val_mae: 226.0284\n"
     ]
    }
   ],
   "source": [
    "# 학습 실시 (hist = )\n",
    "hist = model_dl.fit(train_x, train_y, epochs = 100, validation_data = (test_x, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a7c19767",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, loss_ax = plt.subplots()\n",
    "\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(hist.history['val_loss'], 'r', label='test loss')\n",
    "\n",
    "acc_ax.plot(hist.history['mae'], 'b', label='train mae')\n",
    "acc_ax.plot(hist.history['val_mae'], 'g', label='val mae')\n",
    "\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "acc_ax.set_ylabel('mae')\n",
    "\n",
    "loss_ax.legend(loc='upper left')\n",
    "acc_ax.legend(loc='lower left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a9927723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11406/11414 [============================>.] - ETA: 0s - loss: 156840.6875 - mse: 156840.6875 - mae: 226.6292\n",
      "Epoch 1: val_loss improved from inf to 156320.43750, saving model to my_checkpoint.ckpt\n",
      "11414/11414 [==============================] - 28s 2ms/step - loss: 156870.4219 - mse: 156870.4219 - mae: 226.6376 - val_loss: 156320.4375 - val_mse: 156320.4375 - val_mae: 226.0233\n",
      "Epoch 2/100\n",
      "11408/11414 [============================>.] - ETA: 0s - loss: 156905.2188 - mse: 156905.2188 - mae: 226.6413\n",
      "Epoch 2: val_loss improved from 156320.43750 to 156320.39062, saving model to my_checkpoint.ckpt\n",
      "11414/11414 [==============================] - 28s 2ms/step - loss: 156870.5938 - mse: 156870.5938 - mae: 226.6358 - val_loss: 156320.3906 - val_mse: 156320.3906 - val_mae: 226.0280\n",
      "Epoch 3/100\n",
      "11408/11414 [============================>.] - ETA: 0s - loss: 156885.1875 - mse: 156885.1875 - mae: 226.6307\n",
      "Epoch 3: val_loss improved from 156320.39062 to 156320.15625, saving model to my_checkpoint.ckpt\n",
      "11414/11414 [==============================] - 28s 2ms/step - loss: 156870.5469 - mse: 156870.5469 - mae: 226.6287 - val_loss: 156320.1562 - val_mse: 156320.1562 - val_mae: 226.0432\n",
      "Epoch 4/100\n",
      "11411/11414 [============================>.] - ETA: 0s - loss: 156873.3594 - mse: 156873.3594 - mae: 226.6607\n",
      "Epoch 4: val_loss did not improve from 156320.15625\n",
      "11414/11414 [==============================] - 28s 2ms/step - loss: 156870.2031 - mse: 156870.2031 - mae: 226.6575 - val_loss: 156320.1875 - val_mse: 156320.1875 - val_mae: 226.0397\n",
      "Epoch 5/100\n",
      "11403/11414 [============================>.] - ETA: 0s - loss: 156849.0000 - mse: 156849.0000 - mae: 226.6130\n",
      "Epoch 5: val_loss did not improve from 156320.15625\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 156870.1406 - mse: 156870.1406 - mae: 226.6359 - val_loss: 156320.3281 - val_mse: 156320.3281 - val_mae: 226.0321\n",
      "Epoch 6/100\n",
      "11414/11414 [==============================] - ETA: 0s - loss: 156871.1250 - mse: 156871.1250 - mae: 226.6199\n",
      "Epoch 6: val_loss did not improve from 156320.15625\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 156871.1250 - mse: 156871.1250 - mae: 226.6199 - val_loss: 156320.2969 - val_mse: 156320.2969 - val_mae: 226.0322\n",
      "Epoch 7/100\n",
      "11405/11414 [============================>.] - ETA: 0s - loss: 156870.9844 - mse: 156870.9844 - mae: 226.6407\n",
      "Epoch 7: val_loss did not improve from 156320.15625\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 156870.6875 - mse: 156870.6875 - mae: 226.6399 - val_loss: 156320.1875 - val_mse: 156320.1875 - val_mae: 226.0395\n",
      "Epoch 8/100\n",
      "11400/11414 [============================>.] - ETA: 0s - loss: 156840.5625 - mse: 156840.5625 - mae: 226.6232\n",
      "Epoch 8: val_loss improved from 156320.15625 to 156320.04688, saving model to my_checkpoint.ckpt\n",
      "11414/11414 [==============================] - 30s 3ms/step - loss: 156870.6250 - mse: 156870.6250 - mae: 226.6311 - val_loss: 156320.0469 - val_mse: 156320.0469 - val_mae: 226.0473\n",
      "Epoch 9/100\n",
      "11395/11414 [============================>.] - ETA: 0s - loss: 156855.2812 - mse: 156855.2812 - mae: 226.6191\n",
      "Epoch 9: val_loss improved from 156320.04688 to 156320.01562, saving model to my_checkpoint.ckpt\n",
      "11414/11414 [==============================] - 30s 3ms/step - loss: 156870.3281 - mse: 156870.3281 - mae: 226.6616 - val_loss: 156320.0156 - val_mse: 156320.0156 - val_mae: 226.0533\n",
      "Epoch 10/100\n",
      "11405/11414 [============================>.] - ETA: 0s - loss: 156873.4531 - mse: 156873.4531 - mae: 226.6293\n",
      "Epoch 10: val_loss did not improve from 156320.01562\n",
      "11414/11414 [==============================] - 28s 2ms/step - loss: 156870.6562 - mse: 156870.6562 - mae: 226.6432 - val_loss: 156320.0156 - val_mse: 156320.0156 - val_mae: 226.0508\n",
      "Epoch 11/100\n",
      "11398/11414 [============================>.] - ETA: 0s - loss: 156952.7812 - mse: 156952.7812 - mae: 226.6782\n",
      "Epoch 11: val_loss did not improve from 156320.01562\n",
      "11414/11414 [==============================] - 27s 2ms/step - loss: 156870.0625 - mse: 156870.0625 - mae: 226.6616 - val_loss: 156320.2188 - val_mse: 156320.2188 - val_mae: 226.0385\n",
      "Epoch 12/100\n",
      "11399/11414 [============================>.] - ETA: 0s - loss: 156790.5625 - mse: 156790.5625 - mae: 226.6212\n",
      "Epoch 12: val_loss did not improve from 156320.01562\n",
      "11414/11414 [==============================] - 28s 2ms/step - loss: 156870.5938 - mse: 156870.5938 - mae: 226.6429 - val_loss: 156320.0781 - val_mse: 156320.0781 - val_mae: 226.0450\n",
      "Epoch 13/100\n",
      "11400/11414 [============================>.] - ETA: 0s - loss: 156947.2969 - mse: 156947.2969 - mae: 226.6875\n",
      "Epoch 13: val_loss did not improve from 156320.01562\n",
      "11414/11414 [==============================] - 38s 3ms/step - loss: 156870.2188 - mse: 156870.2188 - mae: 226.6567 - val_loss: 156320.1875 - val_mse: 156320.1875 - val_mae: 226.0396\n",
      "Epoch 14/100\n",
      "11402/11414 [============================>.] - ETA: 0s - loss: 156874.1406 - mse: 156874.1406 - mae: 226.6410\n",
      "Epoch 14: val_loss did not improve from 156320.01562\n",
      "11414/11414 [==============================] - 30s 3ms/step - loss: 156870.5156 - mse: 156870.5156 - mae: 226.6426 - val_loss: 156320.3750 - val_mse: 156320.3750 - val_mae: 226.0307\n",
      "Epoch 15/100\n",
      "11395/11414 [============================>.] - ETA: 0s - loss: 156973.7031 - mse: 156973.7031 - mae: 226.6203\n",
      "Epoch 15: val_loss did not improve from 156320.01562\n",
      "11414/11414 [==============================] - 30s 3ms/step - loss: 156871.1719 - mse: 156871.1719 - mae: 226.6026 - val_loss: 156320.2969 - val_mse: 156320.2969 - val_mae: 226.0322\n",
      "Epoch 16/100\n",
      "11405/11414 [============================>.] - ETA: 0s - loss: 156885.2344 - mse: 156885.2344 - mae: 226.6199\n",
      "Epoch 16: val_loss did not improve from 156320.01562\n",
      "11414/11414 [==============================] - 31s 3ms/step - loss: 156870.9688 - mse: 156870.9688 - mae: 226.6259 - val_loss: 156320.2969 - val_mse: 156320.2969 - val_mae: 226.0322\n",
      "Epoch 17/100\n",
      "11410/11414 [============================>.] - ETA: 0s - loss: 156825.6406 - mse: 156825.6406 - mae: 226.5988\n",
      "Epoch 17: val_loss did not improve from 156320.01562\n",
      "11414/11414 [==============================] - 30s 3ms/step - loss: 156870.7656 - mse: 156870.7656 - mae: 226.6145 - val_loss: 156320.2344 - val_mse: 156320.2344 - val_mae: 226.0390\n",
      "Epoch 18/100\n",
      "11413/11414 [============================>.] - ETA: 0s - loss: 156874.2500 - mse: 156874.2500 - mae: 226.6317\n",
      "Epoch 18: val_loss did not improve from 156320.01562\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 156870.6562 - mse: 156870.6562 - mae: 226.6305 - val_loss: 156320.1562 - val_mse: 156320.1562 - val_mae: 226.0432\n",
      "Epoch 19/100\n",
      "11410/11414 [============================>.] - ETA: 0s - loss: 156895.3750 - mse: 156895.3750 - mae: 226.6355\n",
      "Epoch 19: val_loss improved from 156320.01562 to 156319.95312, saving model to my_checkpoint.ckpt\n",
      "11414/11414 [==============================] - 30s 3ms/step - loss: 156870.2500 - mse: 156870.2500 - mae: 226.6303 - val_loss: 156319.9531 - val_mse: 156319.9531 - val_mae: 226.0559\n",
      "Epoch 20/100\n",
      "11410/11414 [============================>.] - ETA: 0s - loss: 156833.7812 - mse: 156833.7812 - mae: 226.6295\n",
      "Epoch 20: val_loss did not improve from 156319.95312\n",
      "11414/11414 [==============================] - 31s 3ms/step - loss: 156870.7656 - mse: 156870.7656 - mae: 226.6495 - val_loss: 156319.9844 - val_mse: 156319.9844 - val_mae: 226.0682\n",
      "Epoch 21/100\n",
      "11401/11414 [============================>.] - ETA: 0s - loss: 156963.6875 - mse: 156963.6875 - mae: 226.6957\n",
      "Epoch 21: val_loss did not improve from 156319.95312\n",
      "11414/11414 [==============================] - 31s 3ms/step - loss: 156869.8594 - mse: 156869.8594 - mae: 226.6679 - val_loss: 156319.9688 - val_mse: 156319.9688 - val_mae: 226.0626\n",
      "Epoch 22/100\n",
      "11410/11414 [============================>.] - ETA: 0s - loss: 156887.8906 - mse: 156887.8906 - mae: 226.6650\n",
      "Epoch 22: val_loss did not improve from 156319.95312\n",
      "11414/11414 [==============================] - 32s 3ms/step - loss: 156870.0938 - mse: 156870.0938 - mae: 226.6635 - val_loss: 156319.9531 - val_mse: 156319.9531 - val_mae: 226.0683\n",
      "Epoch 23/100\n",
      "11409/11414 [============================>.] - ETA: 0s - loss: 156878.2656 - mse: 156878.2656 - mae: 226.6709\n",
      "Epoch 23: val_loss did not improve from 156319.95312\n",
      "11414/11414 [==============================] - 33s 3ms/step - loss: 156870.6562 - mse: 156870.6562 - mae: 226.6576 - val_loss: 156320.0312 - val_mse: 156320.0312 - val_mae: 226.0692\n",
      "Epoch 24/100\n",
      "11413/11414 [============================>.] - ETA: 0s - loss: 156853.4219 - mse: 156853.4219 - mae: 226.6712\n",
      "Epoch 24: val_loss did not improve from 156319.95312\n",
      "11414/11414 [==============================] - 31s 3ms/step - loss: 156869.7969 - mse: 156869.7969 - mae: 226.6776 - val_loss: 156319.9688 - val_mse: 156319.9688 - val_mae: 226.0598\n",
      "Epoch 25/100\n",
      "11393/11414 [============================>.] - ETA: 0s - loss: 156819.1719 - mse: 156819.1719 - mae: 226.6657\n",
      "Epoch 25: val_loss did not improve from 156319.95312\n",
      "11414/11414 [==============================] - 30s 3ms/step - loss: 156869.6562 - mse: 156869.6562 - mae: 226.6638 - val_loss: 156320.0938 - val_mse: 156320.0938 - val_mae: 226.0473\n",
      "Epoch 26/100\n",
      "11408/11414 [============================>.] - ETA: 0s - loss: 156872.2500 - mse: 156872.2500 - mae: 226.6289\n",
      "Epoch 26: val_loss did not improve from 156319.95312\n",
      "11414/11414 [==============================] - 30s 3ms/step - loss: 156870.5312 - mse: 156870.5312 - mae: 226.6329 - val_loss: 156320.0312 - val_mse: 156320.0312 - val_mae: 226.0483\n",
      "Epoch 27/100\n",
      "11410/11414 [============================>.] - ETA: 0s - loss: 156891.3125 - mse: 156891.3125 - mae: 226.6657\n",
      "Epoch 27: val_loss did not improve from 156319.95312\n",
      "11414/11414 [==============================] - 30s 3ms/step - loss: 156869.9688 - mse: 156869.9688 - mae: 226.6567 - val_loss: 156320.0469 - val_mse: 156320.0469 - val_mae: 226.0482\n",
      "Epoch 28/100\n",
      "11405/11414 [============================>.] - ETA: 0s - loss: 156900.0938 - mse: 156900.0938 - mae: 226.6290\n",
      "Epoch 28: val_loss did not improve from 156319.95312\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 156870.2969 - mse: 156870.2969 - mae: 226.6229 - val_loss: 156320.1875 - val_mse: 156320.1875 - val_mae: 226.0397\n",
      "Epoch 29/100\n",
      "11393/11414 [============================>.] - ETA: 0s - loss: 156938.1406 - mse: 156938.1406 - mae: 226.6690\n",
      "Epoch 29: val_loss did not improve from 156319.95312\n",
      "11414/11414 [==============================] - 30s 3ms/step - loss: 156870.3906 - mse: 156870.3906 - mae: 226.6537 - val_loss: 156320.1719 - val_mse: 156320.1719 - val_mae: 226.0376\n",
      "Epoch 30/100\n",
      "11400/11414 [============================>.] - ETA: 0s - loss: 156856.1250 - mse: 156856.1250 - mae: 226.6126\n",
      "Epoch 30: val_loss did not improve from 156319.95312\n",
      "11414/11414 [==============================] - 30s 3ms/step - loss: 156870.0781 - mse: 156870.0781 - mae: 226.6330 - val_loss: 156320.0156 - val_mse: 156320.0156 - val_mae: 226.0534\n",
      "Epoch 31/100\n",
      "11414/11414 [==============================] - ETA: 0s - loss: 156870.3750 - mse: 156870.3750 - mae: 226.6416\n",
      "Epoch 31: val_loss did not improve from 156319.95312\n",
      "11414/11414 [==============================] - 30s 3ms/step - loss: 156870.3750 - mse: 156870.3750 - mae: 226.6416 - val_loss: 156320.0469 - val_mse: 156320.0469 - val_mae: 226.0436\n",
      "Epoch 32/100\n",
      "11403/11414 [============================>.] - ETA: 0s - loss: 156809.2656 - mse: 156809.2656 - mae: 226.6121\n",
      "Epoch 32: val_loss did not improve from 156319.95312\n",
      "11414/11414 [==============================] - 31s 3ms/step - loss: 156870.5625 - mse: 156870.5625 - mae: 226.6503 - val_loss: 156320.0938 - val_mse: 156320.0938 - val_mae: 226.0474\n",
      "Epoch 33/100\n",
      "11391/11414 [============================>.] - ETA: 0s - loss: 156950.6406 - mse: 156950.6406 - mae: 226.6654\n",
      "Epoch 33: val_loss did not improve from 156319.95312\n",
      "11414/11414 [==============================] - 27s 2ms/step - loss: 156870.4688 - mse: 156870.4688 - mae: 226.6408 - val_loss: 156320.0781 - val_mse: 156320.0781 - val_mae: 226.0477\n",
      "Epoch 34/100\n",
      "11409/11414 [============================>.] - ETA: 0s - loss: 156898.6094 - mse: 156898.6094 - mae: 226.6414\n",
      "Epoch 34: val_loss did not improve from 156319.95312\n",
      "11414/11414 [==============================] - 28s 2ms/step - loss: 156870.7500 - mse: 156870.7500 - mae: 226.6357 - val_loss: 156320.4062 - val_mse: 156320.4062 - val_mae: 226.0262\n",
      "Epoch 35/100\n",
      "11391/11414 [============================>.] - ETA: 0s - loss: 156737.8438 - mse: 156737.8438 - mae: 226.5635\n",
      "Epoch 35: val_loss did not improve from 156319.95312\n",
      "11414/11414 [==============================] - 28s 2ms/step - loss: 156870.8906 - mse: 156870.8906 - mae: 226.6020 - val_loss: 156320.4062 - val_mse: 156320.4062 - val_mae: 226.0233\n",
      "Epoch 36/100\n",
      "11398/11414 [============================>.] - ETA: 0s - loss: 156919.3438 - mse: 156919.3438 - mae: 226.6262\n",
      "Epoch 36: val_loss did not improve from 156319.95312\n",
      "11414/11414 [==============================] - 30s 3ms/step - loss: 156870.7188 - mse: 156870.7188 - mae: 226.6255 - val_loss: 156320.1719 - val_mse: 156320.1719 - val_mae: 226.0362\n",
      "Epoch 37/100\n",
      "11409/11414 [============================>.] - ETA: 0s - loss: 156872.0000 - mse: 156872.0000 - mae: 226.6388\n",
      "Epoch 37: val_loss did not improve from 156319.95312\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 156870.5938 - mse: 156870.5938 - mae: 226.6411 - val_loss: 156320.0625 - val_mse: 156320.0625 - val_mae: 226.0482\n",
      "Epoch 38/100\n",
      "11402/11414 [============================>.] - ETA: 0s - loss: 156905.4219 - mse: 156905.4219 - mae: 226.6484\n",
      "Epoch 38: val_loss did not improve from 156319.95312\n",
      "11414/11414 [==============================] - 31s 3ms/step - loss: 156870.3906 - mse: 156870.3906 - mae: 226.6527 - val_loss: 156320.1719 - val_mse: 156320.1719 - val_mae: 226.0373\n",
      "Epoch 39/100\n",
      "11398/11414 [============================>.] - ETA: 0s - loss: 156937.5000 - mse: 156937.5000 - mae: 226.6533\n",
      "Epoch 39: val_loss did not improve from 156319.95312\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 156870.7656 - mse: 156870.7656 - mae: 226.6403 - val_loss: 156320.0625 - val_mse: 156320.0625 - val_mae: 226.0450\n",
      "Epoch 40/100\n",
      "11393/11414 [============================>.] - ETA: 0s - loss: 156954.3594 - mse: 156954.3594 - mae: 226.6645\n",
      "Epoch 40: val_loss did not improve from 156319.95312\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 156870.0156 - mse: 156870.0156 - mae: 226.6396 - val_loss: 156320.1719 - val_mse: 156320.1719 - val_mae: 226.0432\n",
      "Epoch 41/100\n",
      "11407/11414 [============================>.] - ETA: 0s - loss: 156831.3750 - mse: 156831.3750 - mae: 226.6198\n",
      "Epoch 41: val_loss did not improve from 156319.95312\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 156871.0469 - mse: 156871.0469 - mae: 226.6303 - val_loss: 156320.1875 - val_mse: 156320.1875 - val_mae: 226.0397\n",
      "Epoch 42/100\n",
      "11409/11414 [============================>.] - ETA: 0s - loss: 156894.5156 - mse: 156894.5156 - mae: 226.6224\n",
      "Epoch 42: val_loss did not improve from 156319.95312\n",
      "11414/11414 [==============================] - 30s 3ms/step - loss: 156870.7344 - mse: 156870.7344 - mae: 226.6184 - val_loss: 156320.2969 - val_mse: 156320.2969 - val_mae: 226.0322\n",
      "Epoch 43/100\n",
      "11394/11414 [============================>.] - ETA: 0s - loss: 156923.0156 - mse: 156923.0156 - mae: 226.6428\n",
      "Epoch 43: val_loss did not improve from 156319.95312\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 156870.5000 - mse: 156870.5000 - mae: 226.6442 - val_loss: 156320.0938 - val_mse: 156320.0938 - val_mae: 226.0453\n",
      "Epoch 44/100\n",
      "11397/11414 [============================>.] - ETA: 0s - loss: 156938.7656 - mse: 156938.7656 - mae: 226.6529\n",
      "Epoch 44: val_loss did not improve from 156319.95312\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 156870.5781 - mse: 156870.5781 - mae: 226.6441 - val_loss: 156320.1875 - val_mse: 156320.1875 - val_mae: 226.0374\n",
      "Epoch 45/100\n",
      "11410/11414 [============================>.] - ETA: 0s - loss: 156898.0781 - mse: 156898.0781 - mae: 226.6371\n",
      "Epoch 45: val_loss did not improve from 156319.95312\n",
      "11414/11414 [==============================] - 29s 2ms/step - loss: 156870.6562 - mse: 156870.6562 - mae: 226.6290 - val_loss: 156320.4531 - val_mse: 156320.4531 - val_mae: 226.0233\n",
      "Epoch 46/100\n",
      "11413/11414 [============================>.] - ETA: 0s - loss: 156871.9688 - mse: 156871.9688 - mae: 226.6146\n",
      "Epoch 46: val_loss did not improve from 156319.95312\n",
      "11414/11414 [==============================] - 28s 2ms/step - loss: 156870.7031 - mse: 156870.7031 - mae: 226.6166 - val_loss: 156320.6406 - val_mse: 156320.6406 - val_mae: 226.0141\n",
      "Epoch 47/100\n",
      "11402/11414 [============================>.] - ETA: 0s - loss: 156870.2812 - mse: 156870.2812 - mae: 226.5949\n",
      "Epoch 47: val_loss did not improve from 156319.95312\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 156871.0156 - mse: 156871.0156 - mae: 226.5979 - val_loss: 156320.2969 - val_mse: 156320.2969 - val_mae: 226.0319\n",
      "Epoch 48/100\n",
      "11407/11414 [============================>.] - ETA: 0s - loss: 156795.8750 - mse: 156795.8750 - mae: 226.6002\n",
      "Epoch 48: val_loss did not improve from 156319.95312\n",
      "11414/11414 [==============================] - 30s 3ms/step - loss: 156870.7188 - mse: 156870.7188 - mae: 226.6259 - val_loss: 156320.1719 - val_mse: 156320.1719 - val_mae: 226.0362\n",
      "Epoch 49/100\n",
      "11411/11414 [============================>.] - ETA: 0s - loss: 156887.8750 - mse: 156887.8750 - mae: 226.6323\n",
      "Epoch 49: val_loss did not improve from 156319.95312\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 156870.6094 - mse: 156870.6094 - mae: 226.6268 - val_loss: 156320.2500 - val_mse: 156320.2500 - val_mae: 226.0353\n",
      "Epoch 50/100\n",
      "11401/11414 [============================>.] - ETA: 0s - loss: 156965.0938 - mse: 156965.0938 - mae: 226.6536\n",
      "Epoch 50: val_loss did not improve from 156319.95312\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 156870.7656 - mse: 156870.7656 - mae: 226.6202 - val_loss: 156320.1719 - val_mse: 156320.1719 - val_mae: 226.0362\n",
      "Epoch 51/100\n",
      "11407/11414 [============================>.] - ETA: 0s - loss: 156822.4219 - mse: 156822.4219 - mae: 226.6241\n",
      "Epoch 51: val_loss did not improve from 156319.95312\n",
      "11414/11414 [==============================] - 30s 3ms/step - loss: 156870.6875 - mse: 156870.6875 - mae: 226.6301 - val_loss: 156320.0156 - val_mse: 156320.0156 - val_mae: 226.0533\n",
      "Epoch 52/100\n",
      "11407/11414 [============================>.] - ETA: 0s - loss: 156853.2188 - mse: 156853.2188 - mae: 226.6547\n",
      "Epoch 52: val_loss did not improve from 156319.95312\n",
      "11414/11414 [==============================] - 30s 3ms/step - loss: 156870.0156 - mse: 156870.0156 - mae: 226.6577 - val_loss: 156320.0156 - val_mse: 156320.0156 - val_mae: 226.0589\n",
      "Epoch 53/100\n",
      "11403/11414 [============================>.] - ETA: 0s - loss: 156850.3125 - mse: 156850.3125 - mae: 226.6651\n",
      "Epoch 53: val_loss did not improve from 156319.95312\n",
      "11414/11414 [==============================] - 30s 3ms/step - loss: 156870.0156 - mse: 156870.0156 - mae: 226.6674 - val_loss: 156320.1719 - val_mse: 156320.1719 - val_mae: 226.0432\n",
      "Epoch 54/100\n",
      "11398/11414 [============================>.] - ETA: 0s - loss: 156876.6094 - mse: 156876.6094 - mae: 226.6416\n",
      "Epoch 54: val_loss did not improve from 156319.95312\n",
      "11414/11414 [==============================] - 30s 3ms/step - loss: 156870.4062 - mse: 156870.4062 - mae: 226.6436 - val_loss: 156320.2031 - val_mse: 156320.2031 - val_mae: 226.0395\n",
      "Epoch 55/100\n",
      "11406/11414 [============================>.] - ETA: 0s - loss: 156861.2969 - mse: 156861.2969 - mae: 226.6061\n",
      "Epoch 55: val_loss did not improve from 156319.95312\n",
      "11414/11414 [==============================] - 31s 3ms/step - loss: 156870.7812 - mse: 156870.7812 - mae: 226.6233 - val_loss: 156320.0469 - val_mse: 156320.0469 - val_mae: 226.0444\n",
      "Epoch 56/100\n",
      "11398/11414 [============================>.] - ETA: 0s - loss: 156888.7344 - mse: 156888.7344 - mae: 226.6280\n",
      "Epoch 56: val_loss did not improve from 156319.95312\n",
      "11414/11414 [==============================] - 31s 3ms/step - loss: 156870.3750 - mse: 156870.3750 - mae: 226.6309 - val_loss: 156320.0938 - val_mse: 156320.0938 - val_mae: 226.0479\n",
      "Epoch 57/100\n",
      "11413/11414 [============================>.] - ETA: 0s - loss: 156876.3125 - mse: 156876.3125 - mae: 226.6735\n",
      "Epoch 57: val_loss did not improve from 156319.95312\n",
      "11414/11414 [==============================] - 30s 3ms/step - loss: 156870.3281 - mse: 156870.3281 - mae: 226.6712 - val_loss: 156320.0156 - val_mse: 156320.0156 - val_mae: 226.0483\n",
      "Epoch 58/100\n",
      "11414/11414 [==============================] - ETA: 0s - loss: 156870.1094 - mse: 156870.1094 - mae: 226.6367\n",
      "Epoch 58: val_loss did not improve from 156319.95312\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 156870.1094 - mse: 156870.1094 - mae: 226.6367 - val_loss: 156320.0312 - val_mse: 156320.0312 - val_mae: 226.0509\n",
      "Epoch 59/100\n",
      "11410/11414 [============================>.] - ETA: 0s - loss: 156887.0469 - mse: 156887.0469 - mae: 226.6458\n",
      "Epoch 59: val_loss did not improve from 156319.95312\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 156870.4688 - mse: 156870.4688 - mae: 226.6454 - val_loss: 156320.0938 - val_mse: 156320.0938 - val_mae: 226.0474\n",
      "Epoch 60/100\n",
      "11397/11414 [============================>.] - ETA: 0s - loss: 156882.2969 - mse: 156882.2969 - mae: 226.6691\n",
      "Epoch 60: val_loss did not improve from 156319.95312\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 156870.0469 - mse: 156870.0469 - mae: 226.6564 - val_loss: 156319.9531 - val_mse: 156319.9531 - val_mae: 226.0684\n",
      "Epoch 61/100\n",
      "11403/11414 [============================>.] - ETA: 0s - loss: 156935.2812 - mse: 156935.2812 - mae: 226.6662\n",
      "Epoch 61: val_loss did not improve from 156319.95312\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 156870.5625 - mse: 156870.5625 - mae: 226.6420 - val_loss: 156320.0312 - val_mse: 156320.0312 - val_mae: 226.0605\n",
      "Epoch 62/100\n",
      "11414/11414 [==============================] - ETA: 0s - loss: 156870.1406 - mse: 156870.1406 - mae: 226.6667\n",
      "Epoch 62: val_loss did not improve from 156319.95312\n",
      "11414/11414 [==============================] - 33s 3ms/step - loss: 156870.1406 - mse: 156870.1406 - mae: 226.6667 - val_loss: 156319.9844 - val_mse: 156319.9844 - val_mae: 226.0681\n",
      "Epoch 63/100\n",
      "11411/11414 [============================>.] - ETA: 0s - loss: 156891.4219 - mse: 156891.4219 - mae: 226.6697\n",
      "Epoch 63: val_loss did not improve from 156319.95312\n",
      "11414/11414 [==============================] - 30s 3ms/step - loss: 156870.2031 - mse: 156870.2031 - mae: 226.6633 - val_loss: 156319.9688 - val_mse: 156319.9688 - val_mae: 226.0684\n",
      "Epoch 64/100\n",
      "11391/11414 [============================>.] - ETA: 0s - loss: 156978.9219 - mse: 156978.9219 - mae: 226.6989\n",
      "Epoch 64: val_loss did not improve from 156319.95312\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 156870.3281 - mse: 156870.3281 - mae: 226.6425 - val_loss: 156320.0000 - val_mse: 156320.0000 - val_mae: 226.0685\n",
      "Epoch 65/100\n",
      "11399/11414 [============================>.] - ETA: 0s - loss: 156792.4844 - mse: 156792.4844 - mae: 226.6476\n",
      "Epoch 65: val_loss did not improve from 156319.95312\n",
      "11414/11414 [==============================] - 30s 3ms/step - loss: 156870.6094 - mse: 156870.6094 - mae: 226.6645 - val_loss: 156320.0000 - val_mse: 156320.0000 - val_mae: 226.0589\n",
      "Epoch 66/100\n",
      "11395/11414 [============================>.] - ETA: 0s - loss: 156857.9375 - mse: 156857.9375 - mae: 226.6086\n",
      "Epoch 66: val_loss did not improve from 156319.95312\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 156870.5781 - mse: 156870.5781 - mae: 226.6337 - val_loss: 156320.0938 - val_mse: 156320.0938 - val_mae: 226.0479\n",
      "Epoch 67/100\n",
      "11409/11414 [============================>.] - ETA: 0s - loss: 156894.7031 - mse: 156894.7031 - mae: 226.6434\n",
      "Epoch 67: val_loss improved from 156319.95312 to 156319.93750, saving model to my_checkpoint.ckpt\n",
      "11414/11414 [==============================] - 30s 3ms/step - loss: 156870.4531 - mse: 156870.4531 - mae: 226.6400 - val_loss: 156319.9375 - val_mse: 156319.9375 - val_mae: 226.0559\n",
      "Epoch 68/100\n",
      "11411/11414 [============================>.] - ETA: 0s - loss: 156868.5781 - mse: 156868.5781 - mae: 226.6598\n",
      "Epoch 68: val_loss did not improve from 156319.93750\n",
      "11414/11414 [==============================] - 30s 3ms/step - loss: 156869.8281 - mse: 156869.8281 - mae: 226.6617 - val_loss: 156320.0312 - val_mse: 156320.0312 - val_mae: 226.0605\n",
      "Epoch 69/100\n",
      "11396/11414 [============================>.] - ETA: 0s - loss: 156914.7969 - mse: 156914.7969 - mae: 226.6736\n",
      "Epoch 69: val_loss did not improve from 156319.93750\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 156870.4688 - mse: 156870.4688 - mae: 226.6619 - val_loss: 156320.0156 - val_mse: 156320.0156 - val_mae: 226.0605\n",
      "Epoch 70/100\n",
      "11402/11414 [============================>.] - ETA: 0s - loss: 156966.2656 - mse: 156966.2656 - mae: 226.6875\n",
      "Epoch 70: val_loss did not improve from 156319.93750\n",
      "11414/11414 [==============================] - 31s 3ms/step - loss: 156870.2344 - mse: 156870.2344 - mae: 226.6579 - val_loss: 156320.0156 - val_mse: 156320.0156 - val_mae: 226.0605\n",
      "Epoch 71/100\n",
      "11399/11414 [============================>.] - ETA: 0s - loss: 156696.0781 - mse: 156696.0781 - mae: 226.5973\n",
      "Epoch 71: val_loss did not improve from 156319.93750\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 156870.5000 - mse: 156870.5000 - mae: 226.6570 - val_loss: 156320.0469 - val_mse: 156320.0469 - val_mae: 226.0609\n",
      "Epoch 72/100\n",
      "11413/11414 [============================>.] - ETA: 0s - loss: 156874.2344 - mse: 156874.2344 - mae: 226.6399\n",
      "Epoch 72: val_loss did not improve from 156319.93750\n",
      "11414/11414 [==============================] - 31s 3ms/step - loss: 156870.9375 - mse: 156870.9375 - mae: 226.6409 - val_loss: 156319.9688 - val_mse: 156319.9688 - val_mae: 226.0559\n",
      "Epoch 73/100\n",
      "11397/11414 [============================>.] - ETA: 0s - loss: 156886.9688 - mse: 156886.9688 - mae: 226.6580\n",
      "Epoch 73: val_loss did not improve from 156319.93750\n",
      "11414/11414 [==============================] - 30s 3ms/step - loss: 156870.5938 - mse: 156870.5938 - mae: 226.6412 - val_loss: 156320.0469 - val_mse: 156320.0469 - val_mae: 226.0444\n",
      "Epoch 74/100\n",
      "11394/11414 [============================>.] - ETA: 0s - loss: 156961.3906 - mse: 156961.3906 - mae: 226.6395\n",
      "Epoch 74: val_loss did not improve from 156319.93750\n",
      "11414/11414 [==============================] - 30s 3ms/step - loss: 156870.7656 - mse: 156870.7656 - mae: 226.6185 - val_loss: 156320.0938 - val_mse: 156320.0938 - val_mae: 226.0479\n",
      "Epoch 75/100\n",
      "11411/11414 [============================>.] - ETA: 0s - loss: 156891.4844 - mse: 156891.4844 - mae: 226.6572\n",
      "Epoch 75: val_loss did not improve from 156319.93750\n",
      "11414/11414 [==============================] - 31s 3ms/step - loss: 156870.6562 - mse: 156870.6562 - mae: 226.6500 - val_loss: 156320.0000 - val_mse: 156320.0000 - val_mae: 226.0534\n",
      "Epoch 76/100\n",
      "11397/11414 [============================>.] - ETA: 0s - loss: 156948.0781 - mse: 156948.0781 - mae: 226.6701\n",
      "Epoch 76: val_loss did not improve from 156319.93750\n",
      "11414/11414 [==============================] - 30s 3ms/step - loss: 156870.3438 - mse: 156870.3438 - mae: 226.6525 - val_loss: 156319.9844 - val_mse: 156319.9844 - val_mae: 226.0535\n",
      "Epoch 77/100\n",
      "11401/11414 [============================>.] - ETA: 0s - loss: 156848.6250 - mse: 156848.6250 - mae: 226.6200\n",
      "Epoch 77: val_loss did not improve from 156319.93750\n",
      "11414/11414 [==============================] - 31s 3ms/step - loss: 156870.5000 - mse: 156870.5000 - mae: 226.6264 - val_loss: 156320.0469 - val_mse: 156320.0469 - val_mae: 226.0508\n",
      "Epoch 78/100\n",
      "11401/11414 [============================>.] - ETA: 0s - loss: 156794.7188 - mse: 156794.7188 - mae: 226.6264\n",
      "Epoch 78: val_loss did not improve from 156319.93750\n",
      "11414/11414 [==============================] - 31s 3ms/step - loss: 156870.5000 - mse: 156870.5000 - mae: 226.6235 - val_loss: 156320.3281 - val_mse: 156320.3281 - val_mae: 226.0317\n",
      "Epoch 79/100\n",
      "11414/11414 [==============================] - ETA: 0s - loss: 156871.2344 - mse: 156871.2344 - mae: 226.6015\n",
      "Epoch 79: val_loss did not improve from 156319.93750\n",
      "11414/11414 [==============================] - 30s 3ms/step - loss: 156871.2344 - mse: 156871.2344 - mae: 226.6015 - val_loss: 156320.2188 - val_mse: 156320.2188 - val_mae: 226.0387\n",
      "Epoch 80/100\n",
      "11406/11414 [============================>.] - ETA: 0s - loss: 156881.3750 - mse: 156881.3750 - mae: 226.6456\n",
      "Epoch 80: val_loss did not improve from 156319.93750\n",
      "11414/11414 [==============================] - 30s 3ms/step - loss: 156870.3125 - mse: 156870.3125 - mae: 226.6462 - val_loss: 156320.1406 - val_mse: 156320.1406 - val_mae: 226.0432\n",
      "Epoch 81/100\n",
      "11409/11414 [============================>.] - ETA: 0s - loss: 156889.0625 - mse: 156889.0625 - mae: 226.6373\n",
      "Epoch 81: val_loss did not improve from 156319.93750\n",
      "11414/11414 [==============================] - 31s 3ms/step - loss: 156870.7344 - mse: 156870.7344 - mae: 226.6343 - val_loss: 156320.1719 - val_mse: 156320.1719 - val_mae: 226.0432\n",
      "Epoch 82/100\n",
      "11410/11414 [============================>.] - ETA: 0s - loss: 156899.6250 - mse: 156899.6250 - mae: 226.6652\n",
      "Epoch 82: val_loss did not improve from 156319.93750\n",
      "11414/11414 [==============================] - 32s 3ms/step - loss: 156869.9062 - mse: 156869.9062 - mae: 226.6522 - val_loss: 156320.1719 - val_mse: 156320.1719 - val_mae: 226.0362\n",
      "Epoch 83/100\n",
      "11396/11414 [============================>.] - ETA: 0s - loss: 156941.4375 - mse: 156941.4375 - mae: 226.6713\n",
      "Epoch 83: val_loss did not improve from 156319.93750\n",
      "11414/11414 [==============================] - 30s 3ms/step - loss: 156870.2656 - mse: 156870.2656 - mae: 226.6501 - val_loss: 156320.2500 - val_mse: 156320.2500 - val_mae: 226.0354\n",
      "Epoch 84/100\n",
      "11408/11414 [============================>.] - ETA: 0s - loss: 156900.5312 - mse: 156900.5312 - mae: 226.6371\n",
      "Epoch 84: val_loss did not improve from 156319.93750\n",
      "11414/11414 [==============================] - 30s 3ms/step - loss: 156870.9219 - mse: 156870.9219 - mae: 226.6340 - val_loss: 156320.0000 - val_mse: 156320.0000 - val_mae: 226.0536\n",
      "Epoch 85/100\n",
      "11406/11414 [============================>.] - ETA: 0s - loss: 156894.4062 - mse: 156894.4062 - mae: 226.6424\n",
      "Epoch 85: val_loss did not improve from 156319.93750\n",
      "11414/11414 [==============================] - 30s 3ms/step - loss: 156870.1562 - mse: 156870.1562 - mae: 226.6473 - val_loss: 156320.0781 - val_mse: 156320.0781 - val_mae: 226.0436\n",
      "Epoch 86/100\n",
      "11414/11414 [==============================] - ETA: 0s - loss: 156870.3594 - mse: 156870.3594 - mae: 226.6434\n",
      "Epoch 86: val_loss did not improve from 156319.93750\n",
      "11414/11414 [==============================] - 30s 3ms/step - loss: 156870.3594 - mse: 156870.3594 - mae: 226.6434 - val_loss: 156320.0156 - val_mse: 156320.0156 - val_mae: 226.0508\n",
      "Epoch 87/100\n",
      "11410/11414 [============================>.] - ETA: 0s - loss: 156895.3906 - mse: 156895.3906 - mae: 226.6427\n",
      "Epoch 87: val_loss did not improve from 156319.93750\n",
      "11414/11414 [==============================] - 30s 3ms/step - loss: 156870.5938 - mse: 156870.5938 - mae: 226.6414 - val_loss: 156320.0938 - val_mse: 156320.0938 - val_mae: 226.0453\n",
      "Epoch 88/100\n",
      "11395/11414 [============================>.] - ETA: 0s - loss: 156894.8594 - mse: 156894.8594 - mae: 226.6477\n",
      "Epoch 88: val_loss did not improve from 156319.93750\n",
      "11414/11414 [==============================] - 30s 3ms/step - loss: 156870.0156 - mse: 156870.0156 - mae: 226.6575 - val_loss: 156320.2500 - val_mse: 156320.2500 - val_mae: 226.0390\n",
      "Epoch 89/100\n",
      "11392/11414 [============================>.] - ETA: 0s - loss: 156882.0938 - mse: 156882.0938 - mae: 226.6374\n",
      "Epoch 89: val_loss did not improve from 156319.93750\n",
      "11414/11414 [==============================] - 30s 3ms/step - loss: 156870.5312 - mse: 156870.5312 - mae: 226.6385 - val_loss: 156320.0000 - val_mse: 156320.0000 - val_mae: 226.0515\n",
      "Epoch 90/100\n",
      "11412/11414 [============================>.] - ETA: 0s - loss: 156829.4844 - mse: 156829.4844 - mae: 226.6248\n",
      "Epoch 90: val_loss did not improve from 156319.93750\n",
      "11414/11414 [==============================] - 29s 3ms/step - loss: 156870.4375 - mse: 156870.4375 - mae: 226.6291 - val_loss: 156320.0156 - val_mse: 156320.0156 - val_mae: 226.0533\n",
      "Epoch 91/100\n",
      "11403/11414 [============================>.] - ETA: 0s - loss: 156849.3906 - mse: 156849.3906 - mae: 226.6250\n",
      "Epoch 91: val_loss did not improve from 156319.93750\n",
      "11414/11414 [==============================] - 28s 2ms/step - loss: 156870.7031 - mse: 156870.7031 - mae: 226.6446 - val_loss: 156320.0469 - val_mse: 156320.0469 - val_mae: 226.0689\n",
      "Epoch 92/100\n",
      "11409/11414 [============================>.] - ETA: 0s - loss: 156866.3125 - mse: 156866.3125 - mae: 226.6673\n",
      "Epoch 92: val_loss did not improve from 156319.93750\n",
      "11414/11414 [==============================] - 26s 2ms/step - loss: 156870.2812 - mse: 156870.2812 - mae: 226.6728 - val_loss: 156319.9688 - val_mse: 156319.9688 - val_mae: 226.0625\n",
      "Epoch 93/100\n",
      "11412/11414 [============================>.] - ETA: 0s - loss: 156869.0938 - mse: 156869.0938 - mae: 226.6674\n",
      "Epoch 93: val_loss did not improve from 156319.93750\n",
      "11414/11414 [==============================] - 25s 2ms/step - loss: 156869.8594 - mse: 156869.8594 - mae: 226.6679 - val_loss: 156319.9375 - val_mse: 156319.9375 - val_mae: 226.0860\n",
      "Epoch 94/100\n",
      "11412/11414 [============================>.] - ETA: 0s - loss: 156878.4844 - mse: 156878.4844 - mae: 226.6780\n",
      "Epoch 94: val_loss did not improve from 156319.93750\n",
      "11414/11414 [==============================] - 25s 2ms/step - loss: 156870.0469 - mse: 156870.0469 - mae: 226.6767 - val_loss: 156320.0469 - val_mse: 156320.0469 - val_mae: 226.0689\n",
      "Epoch 95/100\n",
      "11396/11414 [============================>.] - ETA: 0s - loss: 156838.9062 - mse: 156838.9062 - mae: 226.6643\n",
      "Epoch 95: val_loss did not improve from 156319.93750\n",
      "11414/11414 [==============================] - 25s 2ms/step - loss: 156870.3594 - mse: 156870.3594 - mae: 226.6667 - val_loss: 156319.9688 - val_mse: 156319.9688 - val_mae: 226.0559\n",
      "Epoch 96/100\n",
      "11410/11414 [============================>.] - ETA: 0s - loss: 156863.6250 - mse: 156863.6250 - mae: 226.6418\n",
      "Epoch 96: val_loss did not improve from 156319.93750\n",
      "11414/11414 [==============================] - 25s 2ms/step - loss: 156870.3750 - mse: 156870.3750 - mae: 226.6515 - val_loss: 156319.9375 - val_mse: 156319.9375 - val_mae: 226.0559\n",
      "Epoch 97/100\n",
      "11402/11414 [============================>.] - ETA: 0s - loss: 156904.4688 - mse: 156904.4688 - mae: 226.6660\n",
      "Epoch 97: val_loss did not improve from 156319.93750\n",
      "11414/11414 [==============================] - 25s 2ms/step - loss: 156870.8125 - mse: 156870.8125 - mae: 226.6504 - val_loss: 156319.9531 - val_mse: 156319.9531 - val_mae: 226.0628\n",
      "Epoch 98/100\n",
      "11405/11414 [============================>.] - ETA: 0s - loss: 156910.9531 - mse: 156910.9531 - mae: 226.6672\n",
      "Epoch 98: val_loss did not improve from 156319.93750\n",
      "11414/11414 [==============================] - 25s 2ms/step - loss: 156869.9219 - mse: 156869.9219 - mae: 226.6592 - val_loss: 156320.0000 - val_mse: 156320.0000 - val_mae: 226.0589\n",
      "Epoch 99/100\n",
      "11396/11414 [============================>.] - ETA: 0s - loss: 156861.9375 - mse: 156861.9375 - mae: 226.6511\n",
      "Epoch 99: val_loss did not improve from 156319.93750\n",
      "11414/11414 [==============================] - 25s 2ms/step - loss: 156870.1875 - mse: 156870.1875 - mae: 226.6424 - val_loss: 156320.0469 - val_mse: 156320.0469 - val_mae: 226.0514\n",
      "Epoch 100/100\n",
      "11395/11414 [============================>.] - ETA: 0s - loss: 156805.8438 - mse: 156805.8438 - mae: 226.6161\n",
      "Epoch 100: val_loss did not improve from 156319.93750\n",
      "11414/11414 [==============================] - 25s 2ms/step - loss: 156870.2656 - mse: 156870.2656 - mae: 226.6424 - val_loss: 156320.0781 - val_mse: 156320.0781 - val_mae: 226.0474\n"
     ]
    }
   ],
   "source": [
    "# 최적 시점 저장하기 위해  my_checkpoint.ckpt 를 통해 가장 최적 포인트가 저장함.\n",
    "checkpoint_path = \"my_checkpoint.ckpt\"\n",
    "\n",
    "# 모델의 가중치를 저장하는 콜백 만들기\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath = checkpoint_path, save_weights_only=True, save_best_only=True, mointor='val_loss',verbose = 1)\n",
    "\n",
    "\n",
    "# 새로운 콜백으로 모델 훈련하기\n",
    "hist = model_dl.fit(train_x, train_y, epochs = 100, validation_data = (test_x, test_y), callbacks = [cp_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1629e559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2854/2854 - 4s - loss: 156319.9375 - mse: 156319.9375 - mae: 226.0559 - 4s/epoch - 1ms/step\n"
     ]
    }
   ],
   "source": [
    "# 가중치 로드\n",
    "model_dl.load_weights(checkpoint_path)\n",
    "\n",
    "# 모델 재평가\n",
    "loss = model_dl.evaluate(test_x, test_y, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d5f0c75c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2854/2854 [==============================] - 4s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "# y = .predict\n",
    "val_pred = model_dl.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7d2ca8fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "395.3736043722298"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mse 구하기\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(test_y, val_pred, squared = False)\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cd962228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE : 19.884003730944876\n"
     ]
    }
   ],
   "source": [
    "print(\"RMSE :\", mse **0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0d11bdfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.5068502203785528e-05"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# r2 구하기\n",
    "from sklearn.metrics import r2_score\n",
    "r2_score(test_y, val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e0d4a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "soyeon",
   "language": "python",
   "name": "soyeon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
